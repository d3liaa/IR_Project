{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "693dbe99",
   "metadata": {},
   "source": [
    "# Information Retrival Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c06104",
   "metadata": {},
   "source": [
    "Authors: Delia Mennitti - 19610, Letizia Meroi - , Sara Napolitano - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e052f2d3",
   "metadata": {},
   "source": [
    "For this project, we use the **SWIM-IR dataset**, which is described in detail in the paper *“Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval”* by Nandan Thakur, Jianmo Ni, Gustavo Hernández Ábrego, John Wieting, Jimmy Lin, and Daniel Cer.\n",
    "\n",
    "## Task Definition\n",
    "\n",
    "We focus on a **cross-lingual Information Retrieval (IR) task** using the SWIM-IR dataset.\n",
    "\n",
    "Given an English query, the objective is to **retrieve the relevant Wikipedia passage written in another language**. Each query has exactly one associated relevant passage, enabling **automatic and reproducible evaluation** of retrieval performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eda088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from rank_bm25 import BM25Okapi\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15cb6ba",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a538a6",
   "metadata": {},
   "source": [
    "Some stats, bar charts, how is the dataset structured etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83b65b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base data directory\n",
    "BASE_DATA_DIR = \"data/swim_ir_v1/swim_ir_v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d59a4bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '18', 'lang': 'Chinese', 'code': 'zh', 'query': '1869 年，哪个国家发生了一起抢劫案？', 'title': '!Kora Wars', 'text': 'Jan Kivido and Piet Rooi formed a partnership and were the most consistent raiders. The first recorded significant incident between the !Kora people and the colonial government occurred in 1869, when a Griqua and Scottish trader were robbed along the southern bank of the Orange River. Piet Rooi, the leader of another nomadic !Kora group, was held responsible for the robbery, and as punishment was lashed and committed to three months hard labour. He was subsequently released on account of insufficient evidence against him. The treatment he received did not sit well with many of the !Ikora raiders, and this'}\n",
      "{'_id': '39', 'lang': 'Chinese', 'code': 'zh', 'query': '电影《女性艺术革命》是关于什么的？', 'title': '!Women Art Revolution', 'text': 'historians for over 4 decades about their individual and group efforts to help women succeed in the art world and society by helping them overcome obstacles. There were over 40 individuals interviewed for the project. These interviews are done in a variety of places over time. The interviewees talk about their experiences in the art world facing obstacles because of their gender. Many of the artists discuss the works they made as a result.The movie begins with a scene at the Whitney Museum of American Art, where Hershman asks people to name 3 women artists, very few can name more'}\n",
      "{'_id': '45', 'lang': 'Chinese', 'code': 'zh', 'query': '电影《女性艺术革命》讲述了什么？', 'title': '!Women Art Revolution', 'text': \"the radical feminist artists who used activist tactics to get their work shown, demanding parity with their male counterparts. However, by the time queer film historian B. Ruby Rich starts talking about how the lesbian artists didn't want to identify as artists because that label was considered bourgeois by their female counterparts, the movie has taken on an exclusionary air of its own - just like those 'womyn only' coffeehouses that existed 'back in the day.' So, while the film undercuts some of its own arguments by veering too strongly into the very separatist direction it decries - and annoyingly\"}\n",
      "{'_id': '54', 'lang': 'Chinese', 'code': 'zh', 'query': '第一个人类基因组的成本是多少？', 'title': '$1,000 genome', 'text': \"at Baylor College of Medicine, 454 Life Sciences founder Jonathan Rothberg presented James D. Watson with a digital copy of his personal genome sequence on a portable hard drive. Rothberg estimated the cost of the sequence—the first personal genome produced using a next-generation sequencing platform—at $1 million. Watson's genome sequence was published in 2008. A number of scientists have highlighted the cost of additional analysis after performing sequencing. Bruce Korf, past president of the American College of Medical Genetics, described “the $1-million interpretation.” Washington University’s Elaine Mardis prefers “the $100,000 analysis.” At the end of 2007, the biotech company Knome\"}\n",
      "{'_id': '57', 'lang': 'Chinese', 'code': 'zh', 'query': '2014 年 1 月，Illumina 公司推出了哪款测序仪？', 'title': '$1,000 genome', 'text': \"$1,000 genome in a day within 12 months. Sharon Begley wrote: “After years of predictions that the ‘$1,000 genome’ -- a read-out of a person's complete genetic information for about the cost of a dental crown—was just around the corner, a U.S. company is announcing... that it has achieved that milestone.” In January 2014, Illumina launched its HiSeq X Ten Sequencer, claiming to have produced the first $1,000 genome at 30x coverage. Some researchers hailed the HiSeq X Ten's release as a milestone - Michael Schatz of Cold Spring Harbor Laboratory said that “it is a major human accomplishment on\"}\n",
      "{'_id': '92', 'lang': 'Chinese', 'code': 'zh', 'query': '哪位领导人批评总统穆罕默德·布哈里不遵守法院的命令？', 'title': '$2 billion arms deal', 'text': 'for declining the order of the court permitting Dasuki to travel for medical attention. The newspaper described Buhari\\'s action as an attempt to destroy his political opponents like Dasuki with tyrannical methods. In the same vein, Alhaji Tanko Yakasai, a former Liaison Officer to President Shehu Shagari and a founding member of the Arewa Consultative Forum, and Frederick Fasehun, a leader of the Oodua Peoples Congress, berated President Muhammadu Buhari for putting Dasuki\\'s house under siege and for disobeying the court order. Both leaders described Buhari\\'s action as \"deliberate neglect of the rule of law or re-introduction of subtle autocracy'}\n"
     ]
    }
   ],
   "source": [
    "# Full path to Chinese cross-lingual train file\n",
    "zh_path = os.path.join(BASE_DATA_DIR, \"cross_lingual\", \"zh\", \"train.jsonl\")\n",
    "\n",
    "with open(zh_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        print(line.strip())\n",
    "        if i >= 5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd9146a",
   "metadata": {},
   "source": [
    "# Full BM25 baseline for all languages Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cadcfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing monolingual yo\n",
      "Recall@10: 0.8360, MRR@10: 0.7668\n",
      "Processing monolingual ar\n",
      "Recall@10: 0.8720, MRR@10: 0.7583\n",
      "Processing monolingual bn\n",
      "Recall@10: 0.9230, MRR@10: 0.8456\n",
      "Processing monolingual hi\n",
      "Recall@10: 0.9340, MRR@10: 0.8456\n",
      "Processing monolingual de\n",
      "Recall@10: 0.8170, MRR@10: 0.7556\n",
      "Processing monolingual fi\n",
      "Recall@10: 0.8670, MRR@10: 0.7860\n",
      "Processing monolingual id\n",
      "Recall@10: 0.8060, MRR@10: 0.6721\n",
      "Processing monolingual fr\n",
      "Recall@10: 0.6090, MRR@10: 0.5035\n",
      "Processing monolingual es\n",
      "Recall@10: 0.7970, MRR@10: 0.7020\n",
      "Processing monolingual en\n",
      "Recall@10: 0.9390, MRR@10: 0.8804\n",
      "Processing cross_lingual_ext ur\n",
      "Recall@10: 0.1890, MRR@10: 0.1406\n",
      "Processing cross_lingual_ext mai\n",
      "Recall@10: 0.2050, MRR@10: 0.1668\n",
      "Processing cross_lingual_ext mr\n",
      "Recall@10: 0.1780, MRR@10: 0.1384\n",
      "Processing cross_lingual_ext gu\n",
      "Recall@10: 0.1260, MRR@10: 0.0961\n",
      "Processing cross_lingual_ext sa\n",
      "Recall@10: 0.0980, MRR@10: 0.0808\n",
      "Processing cross_lingual_ext mni\n",
      "Recall@10: 0.0110, MRR@10: 0.0039\n",
      "Processing cross_lingual_ext ps\n",
      "Recall@10: 0.1620, MRR@10: 0.1191\n",
      "Processing cross_lingual_ext pa\n",
      "Recall@10: 0.1140, MRR@10: 0.0819\n",
      "Processing cross_lingual_ext ml\n",
      "Recall@10: 0.1160, MRR@10: 0.0872\n",
      "Processing cross_lingual_ext kn\n",
      "Recall@10: 0.1560, MRR@10: 0.1216\n",
      "Processing cross_lingual_ext hi\n",
      "Recall@10: 0.1250, MRR@10: 0.0904\n",
      "Processing cross_lingual_ext as\n",
      "Recall@10: 0.0820, MRR@10: 0.0655\n",
      "Processing cross_lingual_ext bho\n",
      "Recall@10: 0.1810, MRR@10: 0.1470\n",
      "Processing cross_lingual_ext or\n",
      "Recall@10: 0.0900, MRR@10: 0.0622\n",
      "Processing cross_lingual_ext ta\n",
      "Recall@10: 0.2100, MRR@10: 0.1734\n",
      "Processing cross_lingual_ext gom\n",
      "Recall@10: 0.0990, MRR@10: 0.0803\n",
      "Processing cross_lingual sw\n",
      "Recall@10: 0.6720, MRR@10: 0.5531\n",
      "Processing cross_lingual ja\n",
      "Recall@10: 0.5490, MRR@10: 0.3959\n",
      "Processing cross_lingual te\n",
      "Recall@10: 0.2220, MRR@10: 0.1591\n",
      "Processing cross_lingual ru\n",
      "Recall@10: 0.3710, MRR@10: 0.2507\n",
      "Processing cross_lingual yo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/gl/sxzl68dx4b734nhcq_b6ltnc0000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10: 0.5370, MRR@10: 0.4396\n",
      "Processing cross_lingual zh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.263 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10: 0.6120, MRR@10: 0.5277\n",
      "Processing cross_lingual ar\n",
      "Recall@10: 0.3390, MRR@10: 0.2779\n",
      "Processing cross_lingual bn\n",
      "Recall@10: 0.1290, MRR@10: 0.0993\n",
      "Processing cross_lingual hi\n",
      "Recall@10: 0.3520, MRR@10: 0.2805\n",
      "Processing cross_lingual de\n",
      "Recall@10: 0.5730, MRR@10: 0.4634\n",
      "Processing cross_lingual ko\n",
      "Recall@10: 0.3560, MRR@10: 0.2845\n",
      "Processing cross_lingual fi\n",
      "Recall@10: 0.5130, MRR@10: 0.4086\n",
      "Processing cross_lingual id\n",
      "Recall@10: 0.6480, MRR@10: 0.5340\n",
      "Processing cross_lingual fr\n",
      "Recall@10: 0.5780, MRR@10: 0.4417\n",
      "Processing cross_lingual es\n",
      "Recall@10: 0.4250, MRR@10: 0.2943\n",
      "Processing cross_lingual fa\n",
      "Recall@10: 0.6640, MRR@10: 0.5734\n",
      "Processing cross_lingual th\n",
      "Recall@10: 0.6430, MRR@10: 0.5663\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>language</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>MRR@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>monolingual</td>\n",
       "      <td>yo</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.766752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>monolingual</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.758262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>monolingual</td>\n",
       "      <td>bn</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.845633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>monolingual</td>\n",
       "      <td>hi</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.845614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>monolingual</td>\n",
       "      <td>de</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.755586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>monolingual</td>\n",
       "      <td>fi</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.785986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>monolingual</td>\n",
       "      <td>id</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.672081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>monolingual</td>\n",
       "      <td>fr</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.503523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>monolingual</td>\n",
       "      <td>es</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.702014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>monolingual</td>\n",
       "      <td>en</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.880351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cross_lingual_ext</td>\n",
       "      <td>ur</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.140624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cross_lingual_ext</td>\n",
       "      <td>mai</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.166804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cross_lingual_ext</td>\n",
       "      <td>mr</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.138355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cross_lingual_ext</td>\n",
       "      <td>gu</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.096091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cross_lingual_ext</td>\n",
       "      <td>sa</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.080762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cross_lingual_ext</td>\n",
       "      <td>mni</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.003929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cross_lingual_ext</td>\n",
       "      <td>ps</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.119137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cross_lingual_ext</td>\n",
       "      <td>pa</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.081948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cross_lingual_ext</td>\n",
       "      <td>ml</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.087156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cross_lingual_ext</td>\n",
       "      <td>kn</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.121583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cross_lingual_ext</td>\n",
       "      <td>hi</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.090366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cross_lingual_ext</td>\n",
       "      <td>as</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.065546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>cross_lingual_ext</td>\n",
       "      <td>bho</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.146955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cross_lingual_ext</td>\n",
       "      <td>or</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.062191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>cross_lingual_ext</td>\n",
       "      <td>ta</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.173441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>cross_lingual_ext</td>\n",
       "      <td>gom</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.080307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>sw</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.553137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>ja</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.395856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>te</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.159120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>ru</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.250673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>yo</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.439592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>zh</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.527670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.277938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>bn</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.099316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>hi</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.280485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>de</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.463371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>ko</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.284502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>fi</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.408592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>id</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.533985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>fr</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.441651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>es</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.294313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>fa</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.573377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>cross_lingual</td>\n",
       "      <td>th</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.566306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                split language  Recall@10    MRR@10\n",
       "0         monolingual       yo      0.836  0.766752\n",
       "1         monolingual       ar      0.872  0.758262\n",
       "2         monolingual       bn      0.923  0.845633\n",
       "3         monolingual       hi      0.934  0.845614\n",
       "4         monolingual       de      0.817  0.755586\n",
       "5         monolingual       fi      0.867  0.785986\n",
       "6         monolingual       id      0.806  0.672081\n",
       "7         monolingual       fr      0.609  0.503523\n",
       "8         monolingual       es      0.797  0.702014\n",
       "9         monolingual       en      0.939  0.880351\n",
       "10  cross_lingual_ext       ur      0.189  0.140624\n",
       "11  cross_lingual_ext      mai      0.205  0.166804\n",
       "12  cross_lingual_ext       mr      0.178  0.138355\n",
       "13  cross_lingual_ext       gu      0.126  0.096091\n",
       "14  cross_lingual_ext       sa      0.098  0.080762\n",
       "15  cross_lingual_ext      mni      0.011  0.003929\n",
       "16  cross_lingual_ext       ps      0.162  0.119137\n",
       "17  cross_lingual_ext       pa      0.114  0.081948\n",
       "18  cross_lingual_ext       ml      0.116  0.087156\n",
       "19  cross_lingual_ext       kn      0.156  0.121583\n",
       "20  cross_lingual_ext       hi      0.125  0.090366\n",
       "21  cross_lingual_ext       as      0.082  0.065546\n",
       "22  cross_lingual_ext      bho      0.181  0.146955\n",
       "23  cross_lingual_ext       or      0.090  0.062191\n",
       "24  cross_lingual_ext       ta      0.210  0.173441\n",
       "25  cross_lingual_ext      gom      0.099  0.080307\n",
       "26      cross_lingual       sw      0.672  0.553137\n",
       "27      cross_lingual       ja      0.549  0.395856\n",
       "28      cross_lingual       te      0.222  0.159120\n",
       "29      cross_lingual       ru      0.371  0.250673\n",
       "30      cross_lingual       yo      0.537  0.439592\n",
       "31      cross_lingual       zh      0.612  0.527670\n",
       "32      cross_lingual       ar      0.339  0.277938\n",
       "33      cross_lingual       bn      0.129  0.099316\n",
       "34      cross_lingual       hi      0.352  0.280485\n",
       "35      cross_lingual       de      0.573  0.463371\n",
       "36      cross_lingual       ko      0.356  0.284502\n",
       "37      cross_lingual       fi      0.513  0.408592\n",
       "38      cross_lingual       id      0.648  0.533985\n",
       "39      cross_lingual       fr      0.578  0.441651\n",
       "40      cross_lingual       es      0.425  0.294313\n",
       "41      cross_lingual       fa      0.664  0.573377\n",
       "42      cross_lingual       th      0.643  0.566306"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run BM25 for each language folder under each split (cross_lingual, monolingual, etc.)\n",
    "CORPUS_SPLITS = [d for d in os.listdir(BASE_DATA_DIR) if os.path.isdir(os.path.join(BASE_DATA_DIR, d))]\n",
    "MAX_ITEMS = 1000  # use None for full dataset\n",
    "K = 10  # top-K retrieval\n",
    "\n",
    "# Robust JSONL loader\n",
    "def load_jsonl_robust(path, max_items=None):\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_items and i >= max_items:\n",
    "                break\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                obj = ast.literal_eval(line)\n",
    "            data.append(obj)\n",
    "    return data\n",
    "\n",
    "# Tokenizer\n",
    "def tokenize(text, lang_code):\n",
    "    if lang_code == \"zh\":\n",
    "        return list(jieba.cut(text))\n",
    "    else:\n",
    "        return text.lower().split()\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(retrieved, qrels, K=10):\n",
    "    recalls = []\n",
    "    rr_list = []\n",
    "    for qid, top_docs in retrieved.items():\n",
    "        relevant_doc = qrels[qid]\n",
    "        recalls.append(1.0 if relevant_doc in top_docs[:K] else 0.0)\n",
    "        try:\n",
    "            rank = top_docs.index(relevant_doc) + 1\n",
    "            rr_list.append(1.0 / rank)\n",
    "        except ValueError:\n",
    "            rr_list.append(0.0)\n",
    "    return np.mean(recalls), np.mean(rr_list)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "for split in CORPUS_SPLITS:\n",
    "    split_path = os.path.join(BASE_DATA_DIR, split)\n",
    "    for lang_code in os.listdir(split_path):\n",
    "        lang_dir = os.path.join(split_path, lang_code)\n",
    "        lang_path = os.path.join(lang_dir, \"train.jsonl\")\n",
    "        if not os.path.isfile(lang_path):\n",
    "            continue\n",
    "        print(f\"Processing {split} {lang_code}\")\n",
    "        data = load_jsonl_robust(lang_path, max_items=MAX_ITEMS)\n",
    "\n",
    "        # Build documents, queries, qrels\n",
    "        documents = {}\n",
    "        queries = {}\n",
    "        qrels = {}\n",
    "        for item in data:\n",
    "            doc_id = f\"{lang_code}_{item['_id']}\"\n",
    "            lang_field = item.get(\"code\", lang_code)\n",
    "            documents[doc_id] = {\"text\": item.get(\"title\", \"\") + \" \" + item.get(\"text\", \"\"), \"lang\": lang_field}\n",
    "            queries[doc_id] = item.get(\"query\", \"\")\n",
    "            qrels[doc_id] = doc_id\n",
    "\n",
    "        # Tokenize corpus\n",
    "        doc_ids = list(documents.keys())\n",
    "        tokenized_corpus = [tokenize(documents[doc_id][\"text\"], documents[doc_id][\"lang\"]) for doc_id in doc_ids]\n",
    "        if len(tokenized_corpus) == 0:\n",
    "            print(\"No documents found, skipping.\")\n",
    "            continue\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "        # Tokenize queries\n",
    "        tokenized_queries = {qid: tokenize(q, documents[qid][\"lang\"]) for qid, q in queries.items()}\n",
    "\n",
    "        # Retrieve top-K\n",
    "        retrieved = {}\n",
    "        for qid, query_tokens in tokenized_queries.items():\n",
    "            scores = bm25.get_scores(query_tokens)\n",
    "            top_indices = scores.argsort()[-K:][::-1]\n",
    "            retrieved[qid] = [doc_ids[i] for i in top_indices]\n",
    "\n",
    "        # Evaluate\n",
    "        recall, mrr = evaluate(retrieved, qrels, K=K)\n",
    "        results.append({\"split\": split, \"language\": lang_code, \"Recall@10\": recall, \"MRR@10\": mrr})\n",
    "        print(f\"Recall@{K}: {recall:.4f}, MRR@{K}: {mrr:.4f}\")\n",
    "\n",
    "# Display summary table\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce293348",
   "metadata": {},
   "source": [
    "# Dense Multilingual Retrieval Using Embeddings\n",
    "Next step: use LaBSE, mSBERT, or XLM-R embeddings to encode queries and passages.\n",
    "\n",
    "Build a dense vector index (FAISS or similar) and retrieve top-K passages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e8234c",
   "metadata": {},
   "source": [
    "# Hybrid Approach (BM25 + Dense Retrieval)\n",
    "Combine BM25 scores and dense retrieval scores\n",
    "\n",
    "Test weighted combination or re-ranking\n",
    "\n",
    "Compare improvements over BM25-only or dense-only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa97df6",
   "metadata": {},
   "source": [
    "# Neural Reranking on Top-K Results\n",
    "\n",
    "Optional but nice to have\n",
    "\n",
    "Use cross-encoder models to rerank top-K retrieved passages\n",
    "\n",
    "Improves semantic matching on hard queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e3ac7",
   "metadata": {},
   "source": [
    "# Results and Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_bello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
