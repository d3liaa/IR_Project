{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "693dbe99",
   "metadata": {},
   "source": [
    "# Information Retrival Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c06104",
   "metadata": {},
   "source": [
    "Authors: Delia Mennitti - 19610, Letizia Meroi - , Sara Napolitano - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e052f2d3",
   "metadata": {},
   "source": [
    "For this project, we use the **SWIM-IR dataset**, which is described in detail in the paper *“Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval”* by Nandan Thakur, Jianmo Ni, Gustavo Hernández Ábrego, John Wieting, Jimmy Lin, and Daniel Cer.\n",
    "\n",
    "## Task Definition\n",
    "\n",
    "We focus on a **cross-lingual Information Retrieval (IR) task** using the SWIM-IR dataset.\n",
    "\n",
    "Given an English query, the objective is to **retrieve the relevant Wikipedia passage written in another language**. Each query has exactly one associated relevant passage, enabling **automatic and reproducible evaluation** of retrieval performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eda088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from rank_bm25 import BM25Okapi\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15cb6ba",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a538a6",
   "metadata": {},
   "source": [
    "Some stats, bar charts, how is the dataset structured etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83b65b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base data directory\n",
    "BASE_DATA_DIR = \"data/swim_ir_v1/swim_ir_v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d59a4bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '18', 'lang': 'Chinese', 'code': 'zh', 'query': '1869 年，哪个国家发生了一起抢劫案？', 'title': '!Kora Wars', 'text': 'Jan Kivido and Piet Rooi formed a partnership and were the most consistent raiders. The first recorded significant incident between the !Kora people and the colonial government occurred in 1869, when a Griqua and Scottish trader were robbed along the southern bank of the Orange River. Piet Rooi, the leader of another nomadic !Kora group, was held responsible for the robbery, and as punishment was lashed and committed to three months hard labour. He was subsequently released on account of insufficient evidence against him. The treatment he received did not sit well with many of the !Ikora raiders, and this'}\n",
      "{'_id': '39', 'lang': 'Chinese', 'code': 'zh', 'query': '电影《女性艺术革命》是关于什么的？', 'title': '!Women Art Revolution', 'text': 'historians for over 4 decades about their individual and group efforts to help women succeed in the art world and society by helping them overcome obstacles. There were over 40 individuals interviewed for the project. These interviews are done in a variety of places over time. The interviewees talk about their experiences in the art world facing obstacles because of their gender. Many of the artists discuss the works they made as a result.The movie begins with a scene at the Whitney Museum of American Art, where Hershman asks people to name 3 women artists, very few can name more'}\n",
      "{'_id': '45', 'lang': 'Chinese', 'code': 'zh', 'query': '电影《女性艺术革命》讲述了什么？', 'title': '!Women Art Revolution', 'text': \"the radical feminist artists who used activist tactics to get their work shown, demanding parity with their male counterparts. However, by the time queer film historian B. Ruby Rich starts talking about how the lesbian artists didn't want to identify as artists because that label was considered bourgeois by their female counterparts, the movie has taken on an exclusionary air of its own - just like those 'womyn only' coffeehouses that existed 'back in the day.' So, while the film undercuts some of its own arguments by veering too strongly into the very separatist direction it decries - and annoyingly\"}\n",
      "{'_id': '54', 'lang': 'Chinese', 'code': 'zh', 'query': '第一个人类基因组的成本是多少？', 'title': '$1,000 genome', 'text': \"at Baylor College of Medicine, 454 Life Sciences founder Jonathan Rothberg presented James D. Watson with a digital copy of his personal genome sequence on a portable hard drive. Rothberg estimated the cost of the sequence—the first personal genome produced using a next-generation sequencing platform—at $1 million. Watson's genome sequence was published in 2008. A number of scientists have highlighted the cost of additional analysis after performing sequencing. Bruce Korf, past president of the American College of Medical Genetics, described “the $1-million interpretation.” Washington University’s Elaine Mardis prefers “the $100,000 analysis.” At the end of 2007, the biotech company Knome\"}\n",
      "{'_id': '57', 'lang': 'Chinese', 'code': 'zh', 'query': '2014 年 1 月，Illumina 公司推出了哪款测序仪？', 'title': '$1,000 genome', 'text': \"$1,000 genome in a day within 12 months. Sharon Begley wrote: “After years of predictions that the ‘$1,000 genome’ -- a read-out of a person's complete genetic information for about the cost of a dental crown—was just around the corner, a U.S. company is announcing... that it has achieved that milestone.” In January 2014, Illumina launched its HiSeq X Ten Sequencer, claiming to have produced the first $1,000 genome at 30x coverage. Some researchers hailed the HiSeq X Ten's release as a milestone - Michael Schatz of Cold Spring Harbor Laboratory said that “it is a major human accomplishment on\"}\n",
      "{'_id': '92', 'lang': 'Chinese', 'code': 'zh', 'query': '哪位领导人批评总统穆罕默德·布哈里不遵守法院的命令？', 'title': '$2 billion arms deal', 'text': 'for declining the order of the court permitting Dasuki to travel for medical attention. The newspaper described Buhari\\'s action as an attempt to destroy his political opponents like Dasuki with tyrannical methods. In the same vein, Alhaji Tanko Yakasai, a former Liaison Officer to President Shehu Shagari and a founding member of the Arewa Consultative Forum, and Frederick Fasehun, a leader of the Oodua Peoples Congress, berated President Muhammadu Buhari for putting Dasuki\\'s house under siege and for disobeying the court order. Both leaders described Buhari\\'s action as \"deliberate neglect of the rule of law or re-introduction of subtle autocracy'}\n"
     ]
    }
   ],
   "source": [
    "# Full path to Chinese cross-lingual train file\n",
    "zh_path = os.path.join(BASE_DATA_DIR, \"cross_lingual\", \"zh\", \"train.jsonl\")\n",
    "\n",
    "with open(zh_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        print(line.strip())\n",
    "        if i >= 5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd9146a",
   "metadata": {},
   "source": [
    "# Full BM25 baseline for all languages Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cadcfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing language: monolingual\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/swim_ir_v1/swim_ir_v1/monolingual/train.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing language: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m lang_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DATA_DIR, lang_code, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m data \u001b[38;5;241m=\u001b[39m load_jsonl_robust(lang_path, max_items\u001b[38;5;241m=\u001b[39mMAX_ITEMS)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Build documents, queries, qrels\u001b[39;00m\n\u001b[1;32m     52\u001b[0m documents \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mload_jsonl_robust\u001b[0;34m(path, max_items)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_jsonl_robust\u001b[39m(path, max_items\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(f):\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m max_items \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_items:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_bello/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/swim_ir_v1/swim_ir_v1/monolingual/train.jsonl'"
     ]
    }
   ],
   "source": [
    "LANGUAGES = [d for d in os.listdir(BASE_DATA_DIR) if os.path.isdir(os.path.join(BASE_DATA_DIR, d))]\n",
    "MAX_ITEMS = 1000  # use None for full dataset\n",
    "K = 10  # top-K retrieval\n",
    "\n",
    "# Robust JSONL loader\n",
    "def load_jsonl_robust(path, max_items=None):\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_items and i >= max_items:\n",
    "                break\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                obj = ast.literal_eval(line)\n",
    "            data.append(obj)\n",
    "    return data\n",
    "\n",
    "# Tokenizer\n",
    "def tokenize(text, lang_code):\n",
    "    if lang_code == \"zh\":\n",
    "        return list(jieba.cut(text))\n",
    "    else:\n",
    "        return text.lower().split()\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(retrieved, qrels, K=10):\n",
    "    recalls = []\n",
    "    rr_list = []\n",
    "    for qid, top_docs in retrieved.items():\n",
    "        relevant_doc = qrels[qid]\n",
    "        recalls.append(1.0 if relevant_doc in top_docs[:K] else 0.0)\n",
    "        try:\n",
    "            rank = top_docs.index(relevant_doc) + 1\n",
    "            rr_list.append(1.0 / rank)\n",
    "        except ValueError:\n",
    "            rr_list.append(0.0)\n",
    "    return np.mean(recalls), np.mean(rr_list)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "for lang_code in LANGUAGES:\n",
    "    print(f\"\\nProcessing language: {lang_code}\")\n",
    "    lang_path = os.path.join(BASE_DATA_DIR, lang_code, \"train.jsonl\")\n",
    "    data = load_jsonl_robust(lang_path, max_items=MAX_ITEMS)\n",
    "\n",
    "    # Build documents, queries, qrels\n",
    "    documents = {}\n",
    "    queries = {}\n",
    "    qrels = {}\n",
    "    for item in data:\n",
    "        doc_id = f\"{lang_code}_{item['_id']}\"\n",
    "        documents[doc_id] = {\"text\": item[\"title\"] + \" \" + item[\"text\"], \"lang\": item[\"code\"]}\n",
    "        queries[doc_id] = item[\"query\"]\n",
    "        qrels[doc_id] = doc_id\n",
    "\n",
    "    # Tokenize corpus\n",
    "    doc_ids = list(documents.keys())\n",
    "    tokenized_corpus = [tokenize(documents[doc_id][\"text\"], documents[doc_id][\"lang\"]) for doc_id in doc_ids]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    # Tokenize queries\n",
    "    tokenized_queries = {qid: tokenize(q, documents[qid][\"lang\"]) for qid, q in queries.items()}\n",
    "\n",
    "    # Retrieve top-K\n",
    "    retrieved = {}\n",
    "    for qid, query_tokens in tokenized_queries.items():\n",
    "        scores = bm25.get_scores(query_tokens)\n",
    "        top_indices = scores.argsort()[-K:][::-1]\n",
    "        retrieved[qid] = [doc_ids[i] for i in top_indices]\n",
    "\n",
    "    # Evaluate\n",
    "    recall, mrr = evaluate(retrieved, qrels, K=K)\n",
    "    results.append({\"language\": lang_code, \"Recall@10\": recall, \"MRR@10\": mrr})\n",
    "    print(f\"Recall@{K}: {recall:.4f}, MRR@{K}: {mrr:.4f}\")\n",
    "\n",
    "# Display summary table\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce293348",
   "metadata": {},
   "source": [
    "# Dense Multilingual Retrieval Using Embeddings\n",
    "Next step: use LaBSE, mSBERT, or XLM-R embeddings to encode queries and passages.\n",
    "\n",
    "Build a dense vector index (FAISS or similar) and retrieve top-K passages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e8234c",
   "metadata": {},
   "source": [
    "# Hybrid Approach (BM25 + Dense Retrieval)\n",
    "Combine BM25 scores and dense retrieval scores\n",
    "\n",
    "Test weighted combination or re-ranking\n",
    "\n",
    "Compare improvements over BM25-only or dense-only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa97df6",
   "metadata": {},
   "source": [
    "# Neural Reranking on Top-K Results\n",
    "\n",
    "Optional but nice to have\n",
    "\n",
    "Use cross-encoder models to rerank top-K retrieved passages\n",
    "\n",
    "Improves semantic matching on hard queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e3ac7",
   "metadata": {},
   "source": [
    "# Results and Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_bello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
