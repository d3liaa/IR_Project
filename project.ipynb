{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "693dbe99",
   "metadata": {},
   "source": [
    "# Information Retrieval Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c06104",
   "metadata": {},
   "source": [
    "Authors: Delia Mennitti - 19610, Letizia Meroi - 19041, Sara Napolitano - 24656 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50587c1f",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Modern information retrieval systems face increasing challenges in serving multilingual and cross-lingual search needs. While traditional **sparse retrieval methods** like BM25 have been the industry standard for decades, they fundamentally rely on lexical overlap between queries and documents, limiting their effectiveness when dealing with semantic variations, paraphrases, or—most critically—different languages.\n",
    "\n",
    "Recent advances in deep learning and multilingual language models have enabled **dense retrieval methods** that represent texts as continuous semantic vectors in a shared embedding space. These methods promise to capture semantic similarity across languages and overcome the vocabulary mismatch problem inherent to sparse approaches.\n",
    "\n",
    "## Project Objectives\n",
    "\n",
    "This project systematically investigates and compares the effectiveness of dense retrieval versus traditional sparse retrieval for multilingual information retrieval tasks. Specifically, we:\n",
    "\n",
    "1. **Establish a baseline** using **BM25**, the most widely adopted sparse lexical retrieval method\n",
    "2. **Implement dense retrieval** using **LaBSE** (Language-agnostic BERT Sentence Embedding), a state-of-the-art multilingual embedding model\n",
    "3. **Develop a hybrid approach** that combines lexical and semantic signals through score fusion with language-specific optimization\n",
    "4. **Evaluate comprehensively** across 5 diverse languages in both monolingual and cross-lingual settings using standardized metrics (Recall@10 and MRR@10)\n",
    "5. **Analyze language-specific performance** to understand which retrieval paradigm works best for different language families and retrieval scenarios\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "Our analysis addresses three key research questions:\n",
    "\n",
    "- **RQ1**: How does dense semantic retrieval (LaBSE) compare to sparse lexical retrieval (BM25) for monolingual information retrieval?\n",
    "- **RQ2**: What performance gains does dense retrieval provide for cross-lingual tasks where queries and documents are in different languages?\n",
    "- **RQ3**: Can a hybrid approach combining both methods outperform either individual method, and what is the optimal balance between lexical and semantic signals for different languages?\n",
    "\n",
    "## Technical Approach\n",
    "\n",
    "The project implements a complete retrieval pipeline including:\n",
    "- **Data processing** from the SWIM-IR multilingual dataset\n",
    "- **Sparse retrieval** using BM25 with language-specific tokenization\n",
    "- **Dense retrieval** using LaBSE embeddings and cosine similarity search\n",
    "- **Hybrid fusion** with validation-based α tuning for optimal weighting\n",
    "- **Comprehensive evaluation** framework for reproducible experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e052f2d3",
   "metadata": {},
   "source": [
    "# Dataset and Task Description\n",
    "\n",
    "For this project, we use the **SWIM-IR dataset**, which is described in detail in the paper *\"Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval\"* by Nandan Thakur, Jianmo Ni, Gustavo Hernández Ábrego, John Wieting, Jimmy Lin, and Daniel Cer.\n",
    "\n",
    "We focus on a **cross-lingual Information Retrieval (IR) task** using the SWIM-IR dataset.\n",
    "\n",
    "Given an English query, the objective is to **retrieve the relevant Wikipedia passage written in another language**. Each query has exactly one associated relevant passage, enabling **automatic and reproducible evaluation** of retrieval performance.\n",
    "\n",
    "The dataset contains three main splits:\n",
    "- **monolingual**: Query and document in the same language\n",
    "- **cross_lingual**: Query in an other language, document in english (17 languages)\n",
    "- **cross_lingual_ext**: Extended cross-lingual split focusing on Indic languages (16 languages)\n",
    "\n",
    "This setup allows us to evaluate both monolingual and cross-lingual retrieval capabilities of different methods.\n",
    "\n",
    "We selected a diverse set of 5 languages representing different language families:\n",
    "- **German (de)**: Germanic language family\n",
    "- **French (fr)**: Romance language family\n",
    "- **Spanish (es)**: Romance language family\n",
    "- **English (en)**: Germanic language family\n",
    "- **Chinese (zh)**: Sino-Tibetan language family (only cross lingual)\n",
    "\n",
    "This selection provides coverage across both European languages and non-alphabetic writing systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from rank_bm25 import BM25Okapi\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b65b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base data directory\n",
    "BASE_DATA_DIR = \"data/swim_ir_v1/swim_ir_v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15cb6ba",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Before implementing our retrieval methods, we analyze the SWIM-IR dataset to understand its key characteristics: structure, scale, and text properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54de29aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview and corpus statistics\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Get structure and count corpus sizes\n",
    "dataset_structure = {}\n",
    "corpus_sizes = []\n",
    "\n",
    "for split in os.listdir(BASE_DATA_DIR):\n",
    "    split_path = os.path.join(BASE_DATA_DIR, split)\n",
    "    if os.path.isdir(split_path) and not split.startswith('.'):\n",
    "        languages = [d for d in os.listdir(split_path) \n",
    "                    if os.path.isdir(os.path.join(split_path, d)) and not d.startswith('.')]\n",
    "        dataset_structure[split] = sorted(languages)\n",
    "        \n",
    "        # Count items per language\n",
    "        for lang in languages:\n",
    "            lang_path = os.path.join(split_path, lang, \"train.jsonl\")\n",
    "            if os.path.isfile(lang_path):\n",
    "                with open(lang_path, 'r', encoding='utf-8') as f:\n",
    "                    count = sum(1 for _ in f)\n",
    "                corpus_sizes.append({'split': split, 'language': lang, 'num_items': count})\n",
    "\n",
    "df_sizes = pd.DataFrame(corpus_sizes)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SWIM-IR DATASET OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nSplits: {len(dataset_structure)} ({', '.join(dataset_structure.keys())})\")\n",
    "print(f\"Total languages: {len(set([l for ls in dataset_structure.values() for l in ls]))}\")\n",
    "print(f\"Total query-document pairs: {df_sizes['num_items'].sum():,}\")\n",
    "print(f\"\\nBy split:\")\n",
    "for split in dataset_structure:\n",
    "    split_total = df_sizes[df_sizes['split']==split]['num_items'].sum()\n",
    "    print(f\"  {split}: {len(dataset_structure[split])} languages, {split_total:,} pairs\")\n",
    "\n",
    "# Compact visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('SWIM-IR Dataset Structure and Scale', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Split distribution\n",
    "ax1 = axes[0]\n",
    "split_totals = df_sizes.groupby('split')['num_items'].sum()\n",
    "ax1.bar(range(len(split_totals)), split_totals.values, color=['#3498db', '#e74c3c', '#2ecc71'], alpha=0.7)\n",
    "ax1.set_xticks(range(len(split_totals)))\n",
    "ax1.set_xticklabels(split_totals.index, rotation=15, ha='right')\n",
    "ax1.set_ylabel('Query-Document Pairs')\n",
    "ax1.set_title('Corpus Size by Split')\n",
    "for i, v in enumerate(split_totals.values):\n",
    "    ax1.text(i, v, f'{v/1e6:.1f}M', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Top languages\n",
    "ax2 = axes[1]\n",
    "top_10 = df_sizes.nlargest(10, 'num_items')\n",
    "ax2.barh(range(len(top_10)), top_10['num_items']/1000, color='steelblue', alpha=0.7)\n",
    "ax2.set_yticks(range(len(top_10)))\n",
    "ax2.set_yticklabels([f\"{r['language']}\" for _, r in top_10.iterrows()])\n",
    "ax2.set_xlabel('Query-Document Pairs (thousands)')\n",
    "ax2.set_title('Top 10 Languages by Corpus Size')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "SAMPLE_SIZE = 500\n",
    "SELECTED_LANGUAGES_EDA = ['de', 'fr', 'es', 'en', 'zh']\n",
    "CORPUS_SPLITS = [d for d in os.listdir(BASE_DATA_DIR) if os.path.isdir(os.path.join(BASE_DATA_DIR, d))]\n",
    "\n",
    "text_stats = []\n",
    "for split in CORPUS_SPLITS:\n",
    "    split_path = os.path.join(BASE_DATA_DIR, split)\n",
    "    if not os.path.exists(split_path):\n",
    "        continue\n",
    "        \n",
    "    for lang in SELECTED_LANGUAGES_EDA:\n",
    "        lang_path = os.path.join(split_path, lang, \"train.jsonl\")\n",
    "        if not os.path.isfile(lang_path):\n",
    "            continue\n",
    "            \n",
    "        doc_lengths, query_lengths, title_lengths = [], [], []\n",
    "        with open(lang_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= SAMPLE_SIZE:\n",
    "                    break\n",
    "                try:\n",
    "                    item = ast.literal_eval(line.strip())\n",
    "                    doc_lengths.append(len(item.get('text', '').split()))\n",
    "                    query_lengths.append(len(item.get('query', '').split()))\n",
    "                    title_lengths.append(len(item.get('title', '').split()))\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        if doc_lengths:\n",
    "            text_stats.append({\n",
    "                'split': split, 'language': lang,\n",
    "                'avg_doc': np.mean(doc_lengths),\n",
    "                'avg_query': np.mean(query_lengths),\n",
    "                'avg_title': np.mean(title_lengths)\n",
    "            })\n",
    "\n",
    "df_text = pd.DataFrame(text_stats)\n",
    "\n",
    "if len(df_text) > 0:\n",
    "    print(\"=\"*70)\n",
    "    print(\"TEXT LENGTH CHARACTERISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nDocuments: ~{df_text['avg_doc'].mean():.0f} words\")\n",
    "    print(f\"Queries: ~{df_text['avg_query'].mean():.0f} words\")\n",
    "    print(f\"Titles: ~{df_text['avg_title'].mean():.0f} words\")\n",
    "    print(f\"\\nDocuments are ~{df_text['avg_doc'].mean() / df_text['avg_query'].mean():.1f}x longer than queries\")\n",
    "    \n",
    "    # Single compact visualization\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    comparison = df_text.groupby('language')[['avg_doc', 'avg_query']].mean()\n",
    "    comparison.plot(kind='bar', ax=ax, color=['steelblue', 'coral'], width=0.7)\n",
    "    ax.set_ylabel('Average Length (words)')\n",
    "    ax.set_xlabel('Language')\n",
    "    ax.set_title('Document vs Query Length by Language', fontweight='bold')\n",
    "    ax.legend(['Document', 'Query'])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f3fd22",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "### Evaluation Metrics\n",
    "We evaluate all retrieval methods using two standard information retrieval metrics computed at rank $K=10$:\n",
    "- **Recall@10**: The fraction of queries for which the relevant document appears in the top-10 ranked results. This metric measures the system's ability to retrieve relevant documents without requiring them to be ranked first.\n",
    "  \n",
    "  $$\\text{Recall@10} = \\frac{\\text{nr queries with relevant doc in top-10}}{\\text{Total nr queries}}$$\n",
    "\n",
    "- **MRR@10 (Mean Reciprocal Rank)**: The average reciprocal rank of the first relevant document for all queries, capped at rank 10. This metric rewards methods that rank relevant documents higher in the result list.\n",
    "  \n",
    "  $$\\text{MRR@10} = \\frac{1}{|Q|} \\sum_{q=1}^{|Q|} \\frac{1}{\\text{rank}_q}$$\n",
    "  \n",
    "  where $\\text{rank}_q$ is the position of the relevant document for query $q$ (or 0 if not in top-10).\n",
    "\n",
    "Both metrics range from 0 to 1, with higher values indicating better performance.\n",
    "\n",
    "### Retrieval Methods Compared\n",
    "We compare three retrieval approaches:\n",
    "1. **BM25 (Sparse Lexical Retrieval)**: A baseline probabilistic ranking function based on term frequency and inverse document frequency weighting.\n",
    "2. **LaBSE (Dense Semantic Retrieval)**: A multilingual BERT-based embedding model that captures semantic similarity across languages without explicit fine-tuning.\n",
    "3. **Hybrid (Score-based Fusion)**: A weighted combination of BM25 and LaBSE scores, with the weight parameter $\\alpha$ tuned on a validation set to maximize performance.\n",
    "\n",
    "For each method, we retrieve the top 10 documents per query and evaluate against the ground-truth relevant document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd9146a",
   "metadata": {},
   "source": [
    "# 1. Baseline: BM25 (Sparse Lexical Retrieval)\n",
    "\n",
    "We implement **BM25 (Best Matching 25)**, a probabilistic ranking function based on the bag-of-words representation. BM25 is one of the most widely used sparse retrieval methods in information retrieval.\n",
    "\n",
    "**Key characteristics:**\n",
    "- Uses term frequency (TF) and inverse document frequency (IDF) weighting\n",
    "- Language-specific tokenization (including jieba for Chinese)\n",
    "- No semantic understanding - relies purely on lexical overlap\n",
    "- Fast and efficient for large-scale retrieval\n",
    "\n",
    "BM25 serves as our baseline to evaluate the effectiveness of dense retrieval methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c48636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(texts, batch_size=1):\n",
    "    \"\"\"Encode texts to embeddings.\n",
    "    EXTREMELY CONSERVATIVE: batch_size=1 (process one at a time)\n",
    "    \"\"\"\n",
    "    if not globals().get('RUN_DENSE', False):\n",
    "        raise RuntimeError('Dense encoding was disabled (RUN_DENSE=False)')\n",
    "    \n",
    "    if not texts:\n",
    "        raise ValueError('No texts provided for encoding')\n",
    "    \n",
    "    try:\n",
    "        print(f\"  Encoding {len(texts)} texts with batch_size={batch_size}...\")\n",
    "        embeddings = model.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True  # important for cosine similarity\n",
    "        )\n",
    "        # Force garbage collection after encoding\n",
    "        gc.collect()\n",
    "        return embeddings\n",
    "    except MemoryError as me:\n",
    "        print(f'[ERROR] MemoryError during encoding: {me}')\n",
    "        print('[TIP] Memory exhausted - try disabling RUN_DENSE or reducing MAX_ITEMS to 2')\n",
    "        gc.collect()\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] Error during encoding: {e}')\n",
    "        gc.collect()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d2634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global cache for scores\n",
    "SCORE_CACHE = {}  # key: (split, lang) -> dict with bm25_scores, dense_scores, doc_ids, query_ids, qrels\n",
    "\n",
    "# Run BM25 for each language folder under each split (cross_lingual, monolingual, etc.)\n",
    "CORPUS_SPLITS = [d for d in os.listdir(BASE_DATA_DIR) if os.path.isdir(os.path.join(BASE_DATA_DIR, d))]\n",
    "\n",
    "# Fixed set of languages to process (Italian not available in dataset)\n",
    "SELECTED_LANGUAGES = ['de', 'fr', 'es', 'en', 'zh']  # German, French, Spanish, English, Chinese\n",
    "\n",
    "# Start with 5 items - sklearn is more stable than FAISS\n",
    "MAX_ITEMS = 5000 \n",
    "K = 10  # top-K retrieval\n",
    "CANDIDATE_N = 500  # candidates from BM25 and Dense (>= K)\n",
    "\n",
    "# Robust JSONL loader\n",
    "def load_jsonl_robust(path, max_items=MAX_ITEMS):\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_items and i >= max_items:\n",
    "                break\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                obj = ast.literal_eval(line)\n",
    "            data.append(obj)\n",
    "    return data\n",
    "\n",
    "# Tokenizer\n",
    "def tokenize(text, lang_code):\n",
    "    if lang_code == \"zh\":\n",
    "        return list(jieba.cut(text))\n",
    "    else:\n",
    "        return text.lower().split()\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(retrieved, qrels, K=10):\n",
    "    recalls = []\n",
    "    rr_list = []\n",
    "    for qid, top_docs in retrieved.items():\n",
    "        relevant_doc = qrels[qid]\n",
    "        recalls.append(1.0 if relevant_doc in top_docs[:K] else 0.0)\n",
    "        try:\n",
    "            rank = top_docs.index(relevant_doc) + 1\n",
    "            rr_list.append(1.0 / rank)\n",
    "        except ValueError:\n",
    "            rr_list.append(0.0)\n",
    "    return np.mean(recalls), np.mean(rr_list)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "print(f\"Processing selected languages: {', '.join(SELECTED_LANGUAGES)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489cdc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_CACHE_DIR = \"artifacts_cache_bm25\"\n",
    "os.makedirs(BM25_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def bm25_cache_path(split, lang, max_items, k, topn):\n",
    "    return os.path.join(BM25_CACHE_DIR, f\"{split}__{lang}__max{max_items}__k{k}__topn{topn}.json\")\n",
    "\n",
    "def load_bm25_from_disk(split, lang, max_items=MAX_ITEMS, k=K, topn=None):\n",
    "    if topn is None:\n",
    "        topn = max(CANDIDATE_N, K)\n",
    "    path = bm25_cache_path(split, lang, max_items, k, topn)\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_bm25_to_disk(split, lang, payload, max_items=MAX_ITEMS, k=K, topn=None):\n",
    "    if topn is None:\n",
    "        topn = payload.get(\"TOPN\", max(CANDIDATE_N, K))\n",
    "    path = bm25_cache_path(split, lang, max_items, k, topn)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271cfb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put true only if you really need the QxD BM25 matrix\n",
    "STORE_BM25_MATRIX = False  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cadcfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in CORPUS_SPLITS:\n",
    "    split_path = os.path.join(BASE_DATA_DIR, split)\n",
    "    \n",
    "    for lang_code in os.listdir(split_path):\n",
    "        # Only process selected languages\n",
    "        if lang_code not in SELECTED_LANGUAGES:\n",
    "            continue\n",
    "            \n",
    "        lang_dir = os.path.join(split_path, lang_code)\n",
    "        lang_path = os.path.join(lang_dir, \"train.jsonl\")\n",
    "        if not os.path.isfile(lang_path):\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing {split}/{lang_code}\")\n",
    "        print('='*60)\n",
    "\n",
    "        TOPN = max(CANDIDATE_N, K)\n",
    "        cached = load_bm25_from_disk(split, lang_code, MAX_ITEMS, K, TOPN)\n",
    "\n",
    "        if cached is not None:\n",
    "            print(\"BM25 cache hit – loading top-K from disk (skip BM25 computation)\")\n",
    "\n",
    "            # put in RAM cache for later hybrid/rerank\n",
    "            SCORE_CACHE[(split, lang_code)] = SCORE_CACHE.get((split, lang_code), {})\n",
    "            SCORE_CACHE[(split, lang_code)].update(cached)\n",
    "\n",
    "            # Recompute metrics quickly from cached top-K\n",
    "            recall, mrr = evaluate(cached[\"bm25_topk_docs\"], cached[\"qrels\"], K=K)\n",
    "            results.append({\n",
    "                \"split\": split,\n",
    "                \"language\": lang_code,\n",
    "                \"Recall@10\": recall,\n",
    "                \"MRR@10\": mrr,\n",
    "                \"method\": \"BM25\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        data = load_jsonl_robust(lang_path, max_items=MAX_ITEMS)\n",
    "\n",
    "        # Build documents, queries, qrels\n",
    "        documents = {}\n",
    "        queries = {}\n",
    "        qrels = {}\n",
    "        for item in data:\n",
    "            doc_id = f\"{lang_code}_{item['_id']}\"\n",
    "            lang_field = item.get(\"code\", lang_code)\n",
    "            documents[doc_id] = {\"text\": item.get(\"title\", \"\") + \" \" + item.get(\"text\", \"\"), \"lang\": lang_field}\n",
    "            queries[doc_id] = item.get(\"query\", \"\")\n",
    "            qrels[doc_id] = doc_id\n",
    "\n",
    "        # Tokenize corpus\n",
    "        doc_ids = list(documents.keys())\n",
    "        query_ids = list(queries.keys())\n",
    "\n",
    "        tokenized_corpus = [tokenize(documents[doc_id][\"text\"], documents[doc_id][\"lang\"]) for doc_id in doc_ids]\n",
    "\n",
    "        if len(tokenized_corpus) == 0:\n",
    "            print(\"No documents found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "        # Tokenize queries\n",
    "        tokenized_queries = {qid: tokenize(q, documents[qid][\"lang\"]) for qid, q in queries.items()}\n",
    "\n",
    "        # Optional: Build BM25 score matrix QxD\n",
    "        Q = len(query_ids)\n",
    "        D = len(doc_ids)\n",
    "        bm25_scores_matrix = None\n",
    "        if STORE_BM25_MATRIX:\n",
    "            bm25_scores_matrix = np.zeros((Q, D), dtype=np.float32)\n",
    "\n",
    "        # Retrieve top-K\n",
    "        bm25_topk_docs = {}\n",
    "        bm25_topk_scores = {}\n",
    "        retrieved = {}\n",
    "        TOPN = max(CANDIDATE_N, K)\n",
    "        for i, qid in enumerate(query_ids):\n",
    "            scores = np.asarray(bm25.get_scores(tokenized_queries[qid]), dtype=np.float32)\n",
    "            if STORE_BM25_MATRIX:\n",
    "                bm25_scores_matrix[i, :] = scores\n",
    "\n",
    "            top_indices = scores.argsort()[-TOPN:][::-1]\n",
    "            docs = [doc_ids[j] for j in top_indices]\n",
    "            scs  = [float(scores[j]) for j in top_indices]\n",
    "\n",
    "            retrieved[qid] = docs\n",
    "            bm25_topk_docs[qid] = docs\n",
    "            bm25_topk_scores[qid] = scs\n",
    "\n",
    "        # Evaluate\n",
    "        recall, mrr = evaluate(retrieved, qrels, K=K)\n",
    "        results.append({\"split\": split, \"language\": lang_code, \"Recall@10\": recall, \"MRR@10\": mrr, \"method\": \"BM25\"})\n",
    "        print(f\"[OK] BM25 - Recall@{K}: {recall:.4f}, MRR@{K}: {mrr:.4f}\")\n",
    "        \n",
    "        # Save to cache\n",
    "        key = (split, lang_code)\n",
    "        SCORE_CACHE[key] = SCORE_CACHE.get(key, {})\n",
    "        update_pack = {\n",
    "            \"bm25_topk_docs\": bm25_topk_docs,\n",
    "            \"bm25_topk_scores\": bm25_topk_scores,\n",
    "            \"doc_ids\": doc_ids,\n",
    "            \"query_ids\": query_ids,\n",
    "            \"qrels\": qrels\n",
    "        }\n",
    "        if STORE_BM25_MATRIX:\n",
    "            update_pack[\"bm25\"] = bm25_scores_matrix\n",
    "\n",
    "        SCORE_CACHE[key].update(update_pack)\n",
    "\n",
    "\n",
    "        # Save only BM25 top-K to disk, matrix is large and remains in RAM\n",
    "        payload = {\n",
    "            \"doc_ids\": doc_ids,\n",
    "            \"query_ids\": query_ids,\n",
    "            \"qrels\": qrels,\n",
    "            \"bm25_topk_docs\": bm25_topk_docs,\n",
    "            \"bm25_topk_scores\": bm25_topk_scores,\n",
    "            \"TOPN\": int(TOPN)\n",
    "        }\n",
    "        save_bm25_to_disk(split, lang_code, payload, MAX_ITEMS, K, TOPN)\n",
    "        print(\"BM25 top-K cached to disk.\")\n",
    "        gc.collect()\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY RESULTS\")\n",
    "print(\"=\"*60)\n",
    "df_results = pd.DataFrame(results)\n",
    "print(f\"\\nTotal experiments: {len(df_results)}\")\n",
    "\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499f1485",
   "metadata": {},
   "source": [
    "# 2. Dense Retrieval Using Multilingual Embeddings\n",
    "\n",
    "## Approach\n",
    "\n",
    "We implement a **dense retrieval system** using deep learning-based text embeddings. Unlike sparse methods like BM25, dense retrieval represents texts as continuous vectors in a high-dimensional space, enabling semantic similarity matching.\n",
    "\n",
    "## Model Selection: LaBSE\n",
    "\n",
    "We use **LaBSE** (Language-agnostic BERT Sentence Embedding) from the sentence-transformers library, which offers:\n",
    "\n",
    "- **Multilingual support**: Pre-trained on 109 languages\n",
    "- **Cross-lingual alignment**: Semantically similar texts in different languages map to nearby vectors\n",
    "- **768-dimensional embeddings**: Rich semantic representations\n",
    "- **Zero-shot capability**: No fine-tuning required for new language pairs\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "For similarity computation, we use **sklearn's cosine_similarity** instead of FAISS. This approach proved more stable and memory-efficient for our dataset size (1000 items per language), avoiding kernel crashes experienced with FAISS on M3 Mac hardware (16GB RAM).\n",
    "\n",
    "The retrieval process:\n",
    "1. Encode all documents into 768-dim vectors\n",
    "2. Encode all queries into 768-dim vectors  \n",
    "3. Compute cosine similarity matrix (queries × documents)\n",
    "4. Rank documents by similarity scores\n",
    "5. Retrieve top-K results for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfab486d",
   "metadata": {},
   "source": [
    "## Alternative Models Considered\n",
    "\n",
    "Several multilingual embedding models were systematically evaluated for suitability in cross-lingual information retrieval. LaBSE emerged as the preferred choice due to its strong cross-lingual alignment and consistent zero-shot performance across languages, eliminating the need for task-specific fine-tuning. While mSBERT offered advantages in terms of speed and reduced model size, its cross-lingual representations were comparatively weaker, making it less reliable for retrieval accuracy. A fine-tuned XLM-R model was also considered because of its flexibility; however, it requires additional pooling strategies and careful tuning to perform effectively for information retrieval tasks. Overall, LaBSE provided the best balance of performance and operational simplicity for the project’s multilingual requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e8024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense retrieval: Using sklearn instead of FAISS (more stable)\n",
    "RUN_DENSE = True  # set to False to skip dense retrieval\n",
    "\n",
    "if RUN_DENSE:\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        import numpy as np\n",
    "        model = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "        print('[OK] LaBSE model loaded')\n",
    "        print('[OK] Using sklearn cosine_similarity (more stable than FAISS)')\n",
    "    except Exception as e:\n",
    "        print('Failed to load dense model:', e)\n",
    "        RUN_DENSE = False\n",
    "else:\n",
    "    print('Dense retrieval disabled (RUN_DENSE=False)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66040953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, tempfile\n",
    "\n",
    "DENSE_CACHE_DIR = \"artifacts_cache_dense\"\n",
    "os.makedirs(DENSE_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def dense_cache_path(split, lang):\n",
    "    return os.path.join(DENSE_CACHE_DIR, f\"{split}__{lang}__max{MAX_ITEMS}__k{K}.json\")\n",
    "\n",
    "def load_dense_from_disk(split, lang):\n",
    "    path = dense_cache_path(split, lang)\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        # Corrupted file - rename to .bad and treat as cache miss\n",
    "        bad = path + \".bad\"\n",
    "        try:\n",
    "            os.replace(path, bad)\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(f\"[WARN] Dense cache corrupt -> renamed to {bad}. Recompute...\")\n",
    "        return None\n",
    "\n",
    "def save_dense_to_disk(split, lang, payload):\n",
    "    os.makedirs(DENSE_CACHE_DIR, exist_ok=True)\n",
    "    path = dense_cache_path(split, lang)\n",
    "\n",
    "    # Atomic write, avoid partial files\n",
    "    fd, tmp_path = tempfile.mkstemp(dir=DENSE_CACHE_DIR, suffix=\".tmp\")\n",
    "    try:\n",
    "        with os.fdopen(fd, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(payload, f)\n",
    "            f.flush()\n",
    "            os.fsync(f.fileno())\n",
    "        os.replace(tmp_path, path)\n",
    "    finally:\n",
    "        if os.path.exists(tmp_path):\n",
    "            try:\n",
    "                os.remove(tmp_path)\n",
    "            except Exception:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f244f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in CORPUS_SPLITS:\n",
    "    split_path = os.path.join(BASE_DATA_DIR, split)\n",
    "    \n",
    "    for lang_code in os.listdir(split_path):\n",
    "        # Only process selected languages\n",
    "        if lang_code not in SELECTED_LANGUAGES:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing {split}/{lang_code}\")\n",
    "        print('='*60)\n",
    "\n",
    "        # Check cache\n",
    "        cached_dense = load_dense_from_disk(split, lang_code)\n",
    "        if cached_dense is not None:\n",
    "            print(\"Dense cache hit – loading precomputed top-N...\")\n",
    "\n",
    "            key = (split, lang_code)\n",
    "            SCORE_CACHE[key] = SCORE_CACHE.get(key, {})\n",
    "            SCORE_CACHE[key].update(cached_dense)\n",
    "            \n",
    "            # Evaluate cached results\n",
    "            recall, mrr = evaluate(cached_dense[\"dense_topk_docs\"], cached_dense[\"qrels\"], K=K)\n",
    "            results.append({\n",
    "                \"split\": split,\n",
    "                \"language\": lang_code,\n",
    "                \"Recall@10\": recall,\n",
    "                \"MRR@10\": mrr,\n",
    "                \"method\": \"LaBSE\"\n",
    "            })\n",
    "            print(f\"[OK] LaBSE - Recall@{K}: {recall:.4f}, MRR@{K}: {mrr:.4f}\")\n",
    "            continue\n",
    "\n",
    "        print(\"Cache miss – computing scores...\")\n",
    "\n",
    "        lang_dir = os.path.join(split_path, lang_code)\n",
    "        lang_path = os.path.join(lang_dir, \"train.jsonl\")\n",
    "        if not os.path.isfile(lang_path):\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        data = load_jsonl_robust(lang_path, max_items=MAX_ITEMS)\n",
    "\n",
    "        # Build documents, queries, qrels\n",
    "        documents = {}\n",
    "        queries = {}\n",
    "        qrels = {}\n",
    "        for item in data:\n",
    "            doc_id = f\"{lang_code}_{item['_id']}\"\n",
    "            lang_field = item.get(\"code\", lang_code)\n",
    "            documents[doc_id] = {\"text\": item.get(\"title\", \"\") + \" \" + item.get(\"text\", \"\"), \"lang\": lang_field}\n",
    "            queries[doc_id] = item.get(\"query\", \"\")\n",
    "            qrels[doc_id] = doc_id\n",
    "\n",
    "        \n",
    "        doc_ids = list(documents.keys())\n",
    "        query_ids = list(queries.keys())\n",
    "\n",
    "\n",
    "        # Dense retrieval for this language using sklearn (no FAISS!)\n",
    "        if globals().get('RUN_DENSE', False):\n",
    "            try:\n",
    "                # Encode documents\n",
    "                doc_texts = [documents[doc_id][\"text\"] for doc_id in doc_ids]\n",
    "                print(f\"\\n[DENSE] Encoding {len(doc_texts)} documents...\")\n",
    "                doc_embeddings = encode_texts(doc_texts)\n",
    "                print(f\"[OK] Document embeddings shape: {doc_embeddings.shape}\")\n",
    "\n",
    "                # Encode queries\n",
    "                query_id = list(queries.keys())\n",
    "                query_texts = [queries[qid] for qid in query_ids]\n",
    "                print(f\"\\n[DENSE] Encoding {len(query_texts)} queries...\")\n",
    "                query_embeddings = encode_texts(query_texts)\n",
    "                print(f\"[OK] Query embeddings shape: {query_embeddings.shape}\")\n",
    "\n",
    "                # Use sklearn cosine_similarity instead of FAISS\n",
    "                print(f\"\\n[DENSE] Computing cosine similarities...\")\n",
    "                # Compute similarity matrix: queries x documents\n",
    "                similarities = cosine_similarity(query_embeddings, doc_embeddings)\n",
    "                print(f\"[OK] Similarity matrix shape: {similarities.shape}\")\n",
    "                \n",
    "                # Get top-K for each query\n",
    "                dense_topk_docs = {}\n",
    "                dense_topk_scores = {}\n",
    "                TOPN = max(CANDIDATE_N, K)\n",
    "                for i, qid in enumerate(query_ids):\n",
    "                    # Get similarities for this query\n",
    "                    query_sims = similarities[i]\n",
    "                    # Get top-K indices\n",
    "                    top_indices = query_sims.argsort()[-TOPN:][::-1]\n",
    "\n",
    "                    docs = [doc_ids[idx] for idx in top_indices]\n",
    "                    scores = [float(query_sims[idx]) for idx in top_indices]\n",
    "\n",
    "                    dense_topk_docs[qid] = docs\n",
    "                    dense_topk_scores[qid] = scores\n",
    "                    print(f\"  Query {i+1}/{len(query_ids)}: done\")\n",
    "\n",
    "                # Evaluate\n",
    "                recall, mrr = evaluate(dense_topk_docs, qrels, K=K)\n",
    "                results.append({\n",
    "                    \"split\": split,\n",
    "                    \"language\": lang_code,\n",
    "                    \"Recall@10\": recall,\n",
    "                    \"MRR@10\": mrr,\n",
    "                    \"method\": \"LaBSE\"\n",
    "                })\n",
    "                print(f\"\\n[OK] LaBSE - Recall@{K}: {recall:.4f}, MRR@{K}: {mrr:.4f}\")\n",
    "                \n",
    "                # Save dense matrix to cache\n",
    "                dense_payload = {\n",
    "                    \"doc_ids\": doc_ids,\n",
    "                    \"query_ids\": query_ids,\n",
    "                    \"qrels\": qrels,\n",
    "                    \"dense_topk_docs\": dense_topk_docs,\n",
    "                    \"dense_topk_scores\": dense_topk_scores,\n",
    "                    \"TOPN\": int(TOPN)\n",
    "                }\n",
    "\n",
    "                key = (split, lang_code)\n",
    "                SCORE_CACHE[key] = SCORE_CACHE.get(key, {})\n",
    "                SCORE_CACHE[key].update(dense_payload)\n",
    "\n",
    "                save_dense_to_disk(split, lang_code, dense_payload)\n",
    "                print(\"Dense top-N cached to disk.\")\n",
    "\n",
    "\n",
    "\n",
    "                # Clean up\n",
    "                del doc_embeddings, query_embeddings, similarities\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n[ERROR] Dense retrieval failed for {split}/{lang_code}: {e}\")\n",
    "                print(\"Continuing with next language...\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                gc.collect()\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY RESULTS\")\n",
    "print(\"=\"*60)\n",
    "df_results = pd.DataFrame(results)\n",
    "print(f\"\\nTotal experiments: {len(df_results)}\")\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "# copy and freeze results and use them later\n",
    "results_frozen = copy.deepcopy(results)\n",
    "df_results_frozen = df_results.copy(deep=True)\n",
    "\n",
    "# save frozen results with timestamp\n",
    "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "df_results_frozen.to_pickle(f\"df_results_frozen_{ts}.pkl\")\n",
    "\n",
    "print(\"Base results frozen and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f13b9e",
   "metadata": {},
   "source": [
    "# Comparison between BM25 and LaBSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fdcc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and compare BM25 vs LaBSE (Dense BERT-based retrieval)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if 'df_results_frozen' in globals() and not df_results_frozen.empty:\n",
    "    print(\"=\"*70)\n",
    "    print(\"DETAILED RESULTS COMPARISON: BM25 vs LaBSE (Dense BERT)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Show all results\n",
    "    print(\"\\n[RESULTS] All Results:\")\n",
    "    print(df_results_frozen.to_string(index=False))\n",
    "    \n",
    "    # Compare by method\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"AVERAGE PERFORMANCE BY METHOD\")\n",
    "    print(\"=\"*70)\n",
    "    method_avg = df_results_frozen.groupby('method')[['Recall@10', 'MRR@10']].mean()\n",
    "    print(method_avg)\n",
    "    \n",
    "    # Compare by language\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PERFORMANCE BY LANGUAGE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Pivot table for easier comparison\n",
    "    pivot_recall = df_results_frozen.pivot_table(\n",
    "        values='Recall@10', \n",
    "        index=['split', 'language'], \n",
    "        columns='method'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    pivot_mrr = df_results_frozen.pivot_table(\n",
    "        values='MRR@10', \n",
    "        index=['split', 'language'], \n",
    "        columns='method'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    print(\"\\n[METRICS] Recall@10 by Language:\")\n",
    "    print(pivot_recall)\n",
    "    \n",
    "    print(\"\\n[METRICS] MRR@10 by Language:\")\n",
    "    print(pivot_mrr)\n",
    "    \n",
    "    # Count wins\n",
    "    if 'LaBSE' in df_results_frozen['method'].values and 'BM25' in df_results_frozen['method'].values:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"HEAD-TO-HEAD COMPARISON\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Compare on Recall@10\n",
    "        better_recall = (pivot_recall['LaBSE'] > pivot_recall['BM25']).sum()\n",
    "        worse_recall = (pivot_recall['LaBSE'] < pivot_recall['BM25']).sum()\n",
    "        tie_recall = (pivot_recall['LaBSE'] == pivot_recall['BM25']).sum()\n",
    "        \n",
    "        print(f\"\\n[COMPARISON] Recall@10 comparison:\")\n",
    "        print(f\"  LaBSE wins: {better_recall}\")\n",
    "        print(f\"  BM25 wins: {worse_recall}\")\n",
    "        print(f\"  Ties: {tie_recall}\")\n",
    "        \n",
    "        # Compare on MRR@10\n",
    "        better_mrr = (pivot_mrr['LaBSE'] > pivot_mrr['BM25']).sum()\n",
    "        worse_mrr = (pivot_mrr['LaBSE'] < pivot_mrr['BM25']).sum()\n",
    "        tie_mrr = (pivot_mrr['LaBSE'] == pivot_mrr['BM25']).sum()\n",
    "        \n",
    "        print(f\"\\n[COMPARISON] MRR@10 comparison:\")\n",
    "        print(f\"  LaBSE wins: {better_mrr}\")\n",
    "        print(f\"  BM25 wins: {worse_mrr}\")\n",
    "        print(f\"  Ties: {tie_mrr}\")\n",
    "        \n",
    "        print(\"\\n[INFO] LaBSE uses BERT-based dense embeddings (768-dim vectors)\")\n",
    "        print(\"[INFO] sklearn cosine_similarity for retrieval (no FAISS)\")\n",
    "        \n",
    "        # Visualization plots\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"VISUALIZATIONS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Set style\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle('BM25 vs LaBSE Performance Comparison', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Bar chart: Average metrics by method\n",
    "        ax1 = axes[0, 0]\n",
    "        method_avg.plot(kind='bar', ax=ax1, color=['#2ecc71', '#e74c3c'])\n",
    "        ax1.set_title('Average Performance by Method', fontsize=12, fontweight='bold')\n",
    "        ax1.set_xlabel('Method')\n",
    "        ax1.set_ylabel('Score')\n",
    "        ax1.legend(title='Metrics')\n",
    "        ax1.set_xticklabels(ax1.get_xticklabels(), rotation=0)\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 2. Grouped bar chart: MRR@10 by language\n",
    "        ax2 = axes[0, 1]\n",
    "        pivot_mrr.plot(kind='bar', ax=ax2, width=0.8)\n",
    "        ax2.set_title('MRR@10 by Language', fontsize=12, fontweight='bold')\n",
    "        ax2.set_xlabel('Language')\n",
    "        ax2.set_ylabel('MRR@10')\n",
    "        ax2.legend(title='Method')\n",
    "        ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 3. Scatter plot: BM25 vs LaBSE (Recall)\n",
    "        ax3 = axes[1, 0]\n",
    "        for idx, row in pivot_mrr.iterrows():\n",
    "            split, lang = idx if isinstance(idx, tuple) else (idx, idx)\n",
    "            ax3.scatter(row['BM25'], row['LaBSE'], s=100, alpha=0.6)\n",
    "            ax3.annotate(lang, (row['BM25'], row['LaBSE']), \n",
    "                        fontsize=8, ha='right', va='bottom')\n",
    "        \n",
    "        # Add diagonal line\n",
    "        max_val = max(pivot_mrr.max().max(), 1.0)\n",
    "        ax3.plot([0, max_val], [0, max_val], 'k--', alpha=0.3, label='Equal performance')\n",
    "        ax3.set_xlim(0, max_val * 1.05)\n",
    "        ax3.set_ylim(0, max_val * 1.05)\n",
    "        ax3.set_xlabel('BM25 MRR@10')\n",
    "        ax3.set_ylabel('LaBSE MRR@10')\n",
    "        ax3.set_title('BM25 vs LaBSE Performance (MRR@10)', fontsize=12, fontweight='bold')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Win/Loss pie chart\n",
    "        ax4 = axes[1, 1]\n",
    "        win_data = [better_mrr, worse_mrr, tie_mrr]\n",
    "        labels = [f'LaBSE wins\\n({better_mrr})', f'BM25 wins\\n({worse_mrr})', f'Ties\\n({tie_mrr})']\n",
    "        colors = ['#2ecc71', '#e74c3c', '#95a5a6']\n",
    "        wedges, texts, autotexts = ax4.pie(win_data, labels=labels, colors=colors, \n",
    "                                             autopct='%1.1f%%', startangle=90)\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_color('white')\n",
    "            autotext.set_fontweight('bold')\n",
    "        ax4.set_title('Head-to-Head Wins (MRR@10)', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Additional plot: Language-wise comparison\n",
    "        if len(pivot_recall) > 1:\n",
    "            fig2, ax = plt.subplots(figsize=(12, 6))\n",
    "            x = range(len(pivot_recall))\n",
    "            width = 0.35\n",
    "            \n",
    "            ax.bar([i - width/2 for i in x], pivot_recall['BM25'], width, label='BM25', alpha=0.8)\n",
    "            ax.bar([i + width/2 for i in x], pivot_recall['LaBSE'], width, label='LaBSE', alpha=0.8)\n",
    "            \n",
    "            ax.set_xlabel('Language (Split)', fontsize=12)\n",
    "            ax.set_ylabel('Recall', fontsize=12)\n",
    "            ax.set_title('Side-by-Side Comparison: BM25 vs LaBSE', fontsize=14, fontweight='bold')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels([f\"{idx[1]}\\n({idx[0]})\" for idx in pivot_recall.index], \n",
    "                              rotation=45, ha='right')\n",
    "            ax.legend()\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"[WARNING] No results available. Run the main loop (cell 11) first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368872ac",
   "metadata": {},
   "source": [
    "## 2. Hybrid Approach (BM25 + Dense Retrieval)\n",
    "\n",
    "Another valuable direction is to combine the strengths of both sparse and dense methods.\n",
    "\n",
    "**Approach:**\n",
    "- Compute both BM25 scores and LaBSE similarity scores\n",
    "- Combine scores using weighted linear combination: `score = α × BM25 + (1-α) × LaBSE`\n",
    "- Tune α parameter on validation set\n",
    "- Alternative: Use rank fusion methods (Reciprocal Rank Fusion)\n",
    "\n",
    "**Rationale:**\n",
    "- BM25 captures exact lexical matches (good for entity names, technical terms)\n",
    "- Dense retrieval captures semantic similarity (good for paraphrases, cross-lingual)\n",
    "- Hybrid methods often outperform either method alone\n",
    "\n",
    "This approach has shown consistent improvements in recent IR research and TREC competitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88528aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe copy for hybrid experiments\n",
    "results_hybrid = copy.deepcopy(results_frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_N = 500          # candidati da BM25 e Dense (>= K)\n",
    "alpha_grid = np.linspace(0.0, 1.0, 11)\n",
    "seed = 42\n",
    "val_ratio = 0.2\n",
    "\n",
    "def to_score_dict(top_docs, top_scores):\n",
    "    \"\"\"Convert two lists (docs, scores) to {doc: score}.\"\"\"\n",
    "    return {d: float(s) for d, s in zip(top_docs, top_scores)}\n",
    "\n",
    "def minmax_scores(score_map):\n",
    "    \"\"\"\n",
    "    Min-max normalize a dict {doc: score} to [0,1] over its values.\n",
    "    Returns dict {doc: norm_score}.\n",
    "    \"\"\"\n",
    "    if not score_map:\n",
    "        return {}\n",
    "    vals = np.array(list(score_map.values()), dtype=np.float32)\n",
    "    vmin, vmax = float(vals.min()), float(vals.max())\n",
    "    if vmax == vmin:\n",
    "        return {d: 0.0 for d in score_map}\n",
    "    return {d: (float(s) - vmin) / (vmax - vmin) for d, s in score_map.items()}\n",
    "\n",
    "def hybrid_topk_for_query(qid, alpha, pack, K=10, candN=100):\n",
    "    \"\"\"\n",
    "    Candidate-based hybrid for a single query id.\n",
    "    Combines BM25 and Dense scores on the union of their top candN docs.\n",
    "    \"\"\"\n",
    "    bm_docs = pack[\"bm25_topk_docs\"].get(qid, [])[:candN]\n",
    "    bm_scs  = pack[\"bm25_topk_scores\"].get(qid, [])[:candN]\n",
    "    de_docs = pack[\"dense_topk_docs\"].get(qid, [])[:candN]\n",
    "    de_scs  = pack[\"dense_topk_scores\"].get(qid, [])[:candN]\n",
    "\n",
    "    bm_map = to_score_dict(bm_docs, bm_scs)\n",
    "    de_map = to_score_dict(de_docs, de_scs)\n",
    "\n",
    "    cands = set(bm_map) | set(de_map)\n",
    "    if not cands:\n",
    "        return []\n",
    "\n",
    "    # Normalize within candidate set (missing docs get score 0.0)\n",
    "    bm_norm = minmax_scores({d: bm_map.get(d, 0.0) for d in cands})\n",
    "    de_norm = minmax_scores({d: de_map.get(d, 0.0) for d in cands})\n",
    "\n",
    "    # Same formula as full hybrid, but only on candidates\n",
    "    hybrid = [(d, alpha * bm_norm[d] + (1.0 - alpha) * de_norm[d]) for d in cands]\n",
    "    hybrid.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return [d for d, _ in hybrid[:K]]\n",
    "\n",
    "# Assumes you already have evaluate(retrieved, qrels, K)\n",
    "# (the one you posted is fine)\n",
    "\n",
    "# reset to avoid duplicates when re-running the cell\n",
    "results_hybrid = copy.deepcopy(results_frozen) if \"results_frozen\" in globals() else []\n",
    "\n",
    "# drop any previous hybrid rows if present\n",
    "results_hybrid = [r for r in results_hybrid if not str(r.get(\"method\",\"\")).startswith(\"Hybrid\")]\n",
    "\n",
    "\n",
    "alpha_curve_results = []\n",
    "\n",
    "for split in CORPUS_SPLITS:\n",
    "    for lang_code in SELECTED_LANGUAGES:\n",
    "        key = (split, lang_code)\n",
    "        if key not in SCORE_CACHE:\n",
    "            continue\n",
    "\n",
    "        pack = SCORE_CACHE[key]\n",
    "        needed = [\"qrels\", \"bm25_topk_docs\", \"bm25_topk_scores\", \"dense_topk_docs\", \"dense_topk_scores\"]\n",
    "        if any(k not in pack for k in needed):\n",
    "            # Not all components present for hybrid\n",
    "            continue\n",
    "\n",
    "        qrels = pack[\"qrels\"]\n",
    "        query_ids = list(qrels.keys())  # in your dataset qid==docid, so this is fine\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Hybrid tuning for {split}/{lang_code}\")\n",
    "        print('='*60)\n",
    "\n",
    "        # val/test split over query_ids\n",
    "        rng = np.random.default_rng(seed)\n",
    "        shuffled = query_ids.copy()\n",
    "        rng.shuffle(shuffled)\n",
    "        n_val = max(1, int(len(shuffled) * val_ratio))\n",
    "        val_ids = shuffled[:n_val]\n",
    "        test_ids = shuffled[n_val:] if n_val < len(shuffled) else shuffled[:]\n",
    "\n",
    "        best_alpha = None\n",
    "        best_val_mrr = -1.0\n",
    "        best_val_recall = -1.0\n",
    "\n",
    "        # ---- Tune alpha on VAL ----\n",
    "        for alpha in alpha_grid:\n",
    "            retrieved_val = {\n",
    "                qid: hybrid_topk_for_query(qid, float(alpha), pack, K=K, candN=CANDIDATE_N)\n",
    "                for qid in val_ids\n",
    "            }\n",
    "\n",
    "            recall_val, mrr_val = evaluate(retrieved_val, qrels, K=K)\n",
    "\n",
    "            alpha_curve_results.append({\n",
    "                \"split\": split,\n",
    "                \"language\": lang_code,\n",
    "                \"alpha\": float(alpha),\n",
    "                \"val_Recall@10\": float(recall_val),\n",
    "                \"val_MRR@10\": float(mrr_val),\n",
    "                \"val_ratio\": float(val_ratio),\n",
    "                \"seed\": int(seed),\n",
    "                \"candN\": int(CANDIDATE_N),\n",
    "            })\n",
    "\n",
    "            # tie-break: higher MRR, then higher recall, then smaller alpha\n",
    "            if (mrr_val > best_val_mrr) or \\\n",
    "               (mrr_val == best_val_mrr and recall_val > best_val_recall) or \\\n",
    "               (mrr_val == best_val_mrr and recall_val == best_val_recall and (best_alpha is None or alpha < best_alpha)):\n",
    "                best_val_mrr = mrr_val\n",
    "                best_val_recall = recall_val\n",
    "                best_alpha = float(alpha)\n",
    "\n",
    "        print(f\"[TUNE] Best alpha={best_alpha:.2f} on VAL | Recall@{K}={best_val_recall:.4f}, MRR@{K}={best_val_mrr:.4f}\")\n",
    "\n",
    "        # ---- Evaluate on TEST with best alpha ----\n",
    "        retrieved_test = {\n",
    "            qid: hybrid_topk_for_query(qid, best_alpha, pack, K=K, candN=CANDIDATE_N)\n",
    "            for qid in test_ids\n",
    "        }\n",
    "\n",
    "        recall_test, mrr_test = evaluate(retrieved_test, qrels, K=K)\n",
    "\n",
    "        results_hybrid.append({\n",
    "            \"split\": split,\n",
    "            \"language\": lang_code,\n",
    "            \"Recall@10\": float(recall_test),\n",
    "            \"MRR@10\": float(mrr_test),\n",
    "            \"method\": \"Hybrid(tuned)\",\n",
    "            \"alpha\": float(best_alpha),\n",
    "            \"val_Recall@10\": float(best_val_recall),\n",
    "            \"val_MRR@10\": float(best_val_mrr),\n",
    "            \"candN\": int(CANDIDATE_N),\n",
    "        })\n",
    "\n",
    "        print(f\"[OK] Hybrid TEST - alpha={best_alpha:.2f} - Recall@{K}: {recall_test:.4f}, MRR@{K}: {mrr_test:.4f}\")\n",
    "        gc.collect()\n",
    "\n",
    "df_results_hybrid = pd.DataFrame(results_hybrid)\n",
    "df_alpha_curve = pd.DataFrame(alpha_curve_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal rows in df_results_hybrid: {len(df_results_hybrid)}\")\n",
    "\n",
    "# View only the important columns\n",
    "df_results_hybrid[['split', 'language', 'method', 'Recall@10', 'MRR@10']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f1003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    df_alpha_curve,\n",
    "    col=\"language\",\n",
    "    col_wrap=3,        \n",
    "    height=3.5,\n",
    "    sharey=True,\n",
    "    sharex=True\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"alpha\",\n",
    "    y=\"val_MRR@10\",\n",
    "    marker=\"o\",\n",
    "    linewidth=2.5,\n",
    "    markersize=8,\n",
    "    color=\"#2c3e50\"\n",
    ")\n",
    "\n",
    "g.set_axis_labels(r\"$\\alpha$ (BM25 weight)\", \"Validation MRR@10\", fontsize=11, fontweight='bold')\n",
    "g.set_titles(\"Language: {col_name}\", fontsize=12, fontweight='bold')\n",
    "g.figure.suptitle(\n",
    "    \"Hybrid α Parameter Tuning Across Languages\",\n",
    "    fontsize=15,\n",
    "    fontweight=\"bold\",\n",
    "    y=1.00\n",
    ")\n",
    "\n",
    "g.figure.subplots_adjust(wspace=0.3, hspace=0.4)\n",
    "\n",
    "for ax in g.axes.flatten():\n",
    "    ax.grid(alpha=0.3, linestyle='--')\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a87a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('Three Retrieval Methods: Comprehensive Performance Comparison', fontsize=14, fontweight='bold', y=1.00)\n",
    "\n",
    "# Left: Overall metrics comparison\n",
    "ax1 = axes[0]\n",
    "method_order = [\"BM25\", \"LaBSE\", \"Hybrid(tuned)\"]\n",
    "colors_methods = [\"#3498db\", \"#e74c3c\", \"#2ecc71\"]\n",
    "\n",
    "# Get average scores for each method\n",
    "method_stats = df_results_hybrid.groupby('method')[['Recall@10', 'MRR@10']].mean().reindex(method_order)\n",
    "\n",
    "x = np.arange(len(method_order))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, method_stats['Recall@10'], width, label='Recall@10', color='#34495e', alpha=0.85)\n",
    "bars2 = ax1.bar(x + width/2, method_stats['MRR@10'], width, label='MRR@10', color='#e67e22', alpha=0.85)\n",
    "\n",
    "ax1.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Retrieval Method', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Average Performance: All Metrics', fontsize=12, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(method_order, fontsize=11)\n",
    "ax1.legend(fontsize=11, loc='upper left')\n",
    "ax1.set_ylim(0, 1.0)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Right: MRR@10 by method (violin/box style for distribution)\n",
    "ax2 = axes[1]\n",
    "violin_data = [df_results_hybrid[df_results_hybrid['method']==m]['MRR@10'].values for m in method_order]\n",
    "\n",
    "bp = ax2.boxplot(violin_data, labels=method_order, patch_artist=True, widths=0.6)\n",
    "\n",
    "# Color the boxes\n",
    "for patch, color in zip(bp['boxes'], colors_methods):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.85)\n",
    "\n",
    "ax2.set_ylabel('MRR@10', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Retrieval Method', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('MRR@10 Distribution Across Languages', fontsize=12, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_ylim(0, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cdc17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "\n",
    "df_mono = df_results_hybrid[\n",
    "    (df_results_hybrid[\"split\"] == \"monolingual\") &\n",
    "    (df_results_hybrid[\"method\"].isin([\"BM25\", \"LaBSE\", \"Hybrid(tuned)\"]))\n",
    "]\n",
    "\n",
    "sns.barplot(\n",
    "    data=df_mono,\n",
    "    x=\"language\",\n",
    "    y=\"MRR@10\",\n",
    "    hue=\"method\",\n",
    "    hue_order=[\"BM25\", \"LaBSE\", \"Hybrid(tuned)\"],\n",
    "    palette=[\"#3498db\", \"#e74c3c\", \"#2ecc71\"],\n",
    "    width=0.7,\n",
    "    errorbar=None,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Language\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"MRR@10\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Monolingual Retrieval Performance by Language\", fontsize=13, fontweight='bold')\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.legend(title=\"Method\", fontsize=11, title_fontsize=12, loc='lower right', framealpha=0.95)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95196524",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "\n",
    "df_cross = df_results_hybrid[\n",
    "    (df_results_hybrid[\"split\"] == \"cross_lingual\") &\n",
    "    (df_results_hybrid[\"method\"].isin([\"BM25\", \"LaBSE\", \"Hybrid(tuned)\"]))\n",
    "]\n",
    "\n",
    "sns.barplot(\n",
    "    data=df_cross,\n",
    "    x=\"language\",\n",
    "    y=\"MRR@10\",\n",
    "    hue=\"method\",\n",
    "    hue_order=[\"BM25\", \"LaBSE\", \"Hybrid(tuned)\"],\n",
    "    palette=[\"#3498db\", \"#e74c3c\", \"#2ecc71\"],\n",
    "    width=0.7,\n",
    "    errorbar=None,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Language\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"MRR@10\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Cross-Lingual Retrieval Performance by Language\", fontsize=13, fontweight='bold')\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.legend(title=\"Method\", fontsize=11, title_fontsize=12, loc='lower right', framealpha=0.95)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0d428",
   "metadata": {},
   "source": [
    "## Neural Reranking of Hybrid Top-K Candidates\n",
    "\n",
    "The following part implements a **two-stage retrieval + neural reranking pipeline**.\n",
    "\n",
    "**Stage 1 — Candidate generation (Hybrid retrieval):**\n",
    "For each query, it first builds a candidate set using a tuned hybrid ranker that combines lexical signals (BM25) and semantic signals (dense embeddings) with a mixing weight α, previously tuned. This stage is designed to be efficient and to retrieve a reasonably large shortlist (e.g., top-100) from which the true relevant document is likely to appear.\n",
    "\n",
    "**Stage 2 — Reranking with a Cross-Encoder:**\n",
    "For each query, the code reranks the shortlisted documents using a multilingual CrossEncoder [amberoad/bert-multilingual-passage-reranking-msmarco](https://huggingface.co/amberoad/bert-multilingual-passage-reranking-msmarco). The cross-encoder scores each (query, document) pair jointly, allowing deep token-level interactions. To keep CPU inference feasible, it uses batching, limits PyTorch threads, restricts maximum sequence length, and truncates documents.\n",
    "\n",
    "Finally, it evaluates the reranked top results using Recall@10 and MRR@10, and stores results in a frozen CSV.\n",
    "\n",
    "**Benefits:** \n",
    "- Higher ranking quality (especially MRR@10): the cross-encoder models query–document token interactions directly, improving the ordering of top results.\n",
    "- Efficiency at scale: expensive neural scoring is applied only to a small candidate set (Top-K), not the whole collection.\n",
    "- Better multilingual robustness: a multilingual reranker can handle cross-lingual mismatches (vocabulary/phrasing differences) more effectively than pure lexical retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1cff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Limits the number of threads for PyTorch to avoid CPU overload\n",
    "try:\n",
    "    import torch\n",
    "    torch.set_num_threads(min(4, os.cpu_count() or 1))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Multilingual reranker model\n",
    "reranker = CrossEncoder(\n",
    "    \"amberoad/bert-multilingual-passage-reranking-msmarco\",\n",
    "    device=\"cpu\",\n",
    "    max_length=256  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def truncate_text(text, max_chars=400):\n",
    "    \"\"\"\n",
    "    Truncate text to max_chars, cutting at last space before limit.\n",
    "    Benefits: \n",
    "    - preserves whole words\n",
    "    - avoids cutting in the middle of a word\n",
    "    - avoids very short truncations if no space found\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.strip()\n",
    "    if len(t) <= max_chars:\n",
    "        return t\n",
    "    # cut at last space before max_chars\n",
    "    cut = t[:max_chars]\n",
    "    last_space = cut.rfind(\" \")\n",
    "    return cut if last_space < 50 else cut[:last_space]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da0bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_topk_crossencoder(\n",
    "    reranker,\n",
    "    query_text,\n",
    "    doc_ids,\n",
    "    documents,\n",
    "    batch_size=8,\n",
    "    show_progress=False\n",
    "):\n",
    "    # Select only valid documents with non-empty text\n",
    "    valid_doc_ids = []\n",
    "    valid_texts = []\n",
    "\n",
    "    for d in doc_ids:\n",
    "        dt = documents.get(d, \"\")\n",
    "        if isinstance(dt, str) and dt.strip() != \"\":\n",
    "            valid_doc_ids.append(d)\n",
    "            valid_texts.append((query_text, dt))\n",
    "\n",
    "    # If no valid documents, return empty\n",
    "    if not valid_doc_ids:\n",
    "        return [], []\n",
    "\n",
    "    # Predict - the model returns LOGITS [non_relevant, relevant]\n",
    "    logits = reranker.predict(\n",
    "        valid_texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=show_progress\n",
    "    )\n",
    "\n",
    "    logits = np.asarray(logits, dtype=np.float32)\n",
    "    \n",
    "    # Manage different logits to scores\n",
    "    if logits.ndim == 2 and logits.shape[1] == 2:\n",
    "        # calculate probabilities from logits (2 classes) using softmax\n",
    "        exp_logits = np.exp(logits)\n",
    "        probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n",
    "        scores = probs[:, 1] # take the \"relevant\" class probability\n",
    "    else:\n",
    "        # Fallback: if single logit per pair, use it directly\n",
    "        scores = logits.flatten()\n",
    "    \n",
    "    scores = scores.flatten()\n",
    "    \n",
    "    # Ensure lengths match using minimum length\n",
    "    if len(scores) != len(valid_doc_ids):\n",
    "        min_len = min(len(scores), len(valid_doc_ids))\n",
    "        scores = scores[:min_len]\n",
    "        valid_doc_ids = valid_doc_ids[:min_len]\n",
    "\n",
    "    # if we still have no valid scores/docs, return empty\n",
    "    if len(scores) == 0 or len(valid_doc_ids) == 0:\n",
    "        return [], []\n",
    "\n",
    "    # Order by descending scores, first get highest scores\n",
    "    order = np.argsort(-scores).tolist()\n",
    "\n",
    "    # Apply the order to valid_doc_ids\n",
    "    reranked_doc_ids = [valid_doc_ids[i] for i in order]\n",
    "    reranked_scores  = [float(scores[i]) for i in order]\n",
    "\n",
    "    return reranked_doc_ids, reranked_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1a9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_language_prefix(x):\n",
    "    \"\"\"\n",
    "    'de_1' -> '1'\n",
    "    'en_english_22626#23' -> 'english_22626#23' (if present)\n",
    "    If no prefix, returns as-is.\n",
    "    \"\"\"\n",
    "    x = str(x)\n",
    "    if \"_\" in x:\n",
    "        a, b = x.split(\"_\", 1)\n",
    "        if len(a) in (2, 3):  \n",
    "            return b\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4175dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dictqrels(retrieved, qrels_eval, K=10):\n",
    "    \"\"\"\n",
    "    retrieved: {qid: [doc1, doc2, ...]}  (doc ids CLEAN)\n",
    "    qrels_eval: {qid: {docid: rel_int}}\n",
    "    \"\"\"\n",
    "    recalls = []\n",
    "    rr_list = []\n",
    "\n",
    "    for qid, top_docs in retrieved.items():\n",
    "        rels = qrels_eval.get(qid, {})\n",
    "        if not rels:\n",
    "            recalls.append(0.0)\n",
    "            rr_list.append(0.0)\n",
    "            continue\n",
    "\n",
    "        rel_set = {d for d, r in rels.items() if r > 0}\n",
    "\n",
    "        # Recall@K\n",
    "        hit = any(d in rel_set for d in top_docs[:K])\n",
    "        recalls.append(1.0 if hit else 0.0)\n",
    "\n",
    "        # MRR@K\n",
    "        rr = 0.0\n",
    "        for rank, d in enumerate(top_docs[:K], start=1):\n",
    "            if d in rel_set:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        rr_list.append(rr)\n",
    "\n",
    "    return float(np.mean(recalls)), float(np.mean(rr_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b54117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_qrels_single_id(qrels):\n",
    "    \"\"\"\n",
    "    Your qrels: { 'de_1': 'de_1', ... }\n",
    "    Convert to: { 'de_1': {'1': 1} }  (doc ids CLEAN)\n",
    "    Keep qid keys ORIGINAL (with prefix), because evaluate() expects them.\n",
    "    \"\"\"\n",
    "    qrels_eval = {}\n",
    "    for qid, rel_doc in qrels.items():\n",
    "        qid_original = str(qid)\n",
    "        rel_doc_clean = strip_language_prefix(rel_doc)\n",
    "        qrels_eval[qid_original] = {rel_doc_clean: 1}\n",
    "    return qrels_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sanity_check(\n",
    "    queries_map,\n",
    "    queries_map_clean,\n",
    "    docs_map_clean,\n",
    "    qrels,\n",
    "    pack,\n",
    "    alpha,\n",
    "    K_RERANK=50,\n",
    "    CANDIDATE_N=200\n",
    "):\n",
    "    print(\"\\n==================== SANITY CHECK ====================\")\n",
    "\n",
    "    missing_queries = []\n",
    "    missing_rel_docs = []\n",
    "    hybrid_docs_missing = []\n",
    "\n",
    "    for qid_original, rel_doc_original in qrels.items():\n",
    "        qid_clean = strip_language_prefix(qid_original)\n",
    "        rel_doc_clean = strip_language_prefix(rel_doc_original)\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # 1. Check: query text exists\n",
    "        # ---------------------------------------------------\n",
    "        qtext = queries_map.get(qid_original) or queries_map_clean.get(qid_clean)\n",
    "        if not qtext:\n",
    "            missing_queries.append((qid_original, qid_clean))\n",
    "        \n",
    "        # ---------------------------------------------------\n",
    "        # 2. Check: relevant doc exists in docs_map_clean\n",
    "        # ---------------------------------------------------\n",
    "        if rel_doc_clean not in docs_map_clean:\n",
    "            missing_rel_docs.append((qid_original, rel_doc_clean))\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # 3. Check: docs returned by HYBRID exist in docs_map_clean\n",
    "        # ---------------------------------------------------\n",
    "        top_docs = hybrid_topk_for_query(\n",
    "            qid=qid_original,\n",
    "            alpha=alpha,\n",
    "            pack=pack,\n",
    "            K=K_RERANK,\n",
    "            candN=CANDIDATE_N\n",
    "        )\n",
    "        \n",
    "        top_docs_clean = [strip_language_prefix(d) for d in top_docs]\n",
    "\n",
    "        for d in top_docs_clean:\n",
    "            if d not in docs_map_clean:\n",
    "                hybrid_docs_missing.append((qid_original, d))\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Print results\n",
    "    # ---------------------------------------------------\n",
    "    print(\"\\n--- Missing queries (no text found) ---\")\n",
    "    if missing_queries:\n",
    "        for q in missing_queries[:10]:\n",
    "            print(\" \", q)\n",
    "        print(f\"TOTAL missing queries: {len(missing_queries)}\")\n",
    "    else:\n",
    "        print(\"OK ✓\")\n",
    "\n",
    "    print(\"\\n--- Relevant documents missing in docs_map_clean ---\")\n",
    "    if missing_rel_docs:\n",
    "        for q in missing_rel_docs[:10]:\n",
    "            print(\" \", q)\n",
    "        print(f\"TOTAL missing relevant docs: {len(missing_rel_docs)}\")\n",
    "    else:\n",
    "        print(\"OK ✓\")\n",
    "\n",
    "    print(\"\\n--- Docs returned by hybrid but NOT in docs_map_clean ---\")\n",
    "    if hybrid_docs_missing:\n",
    "        for q in hybrid_docs_missing[:10]:\n",
    "            print(\" \", q)\n",
    "        print(f\"TOTAL hybrid docs missing: {len(hybrid_docs_missing)}\")\n",
    "    else:\n",
    "        print(\"OK ✓\")\n",
    "\n",
    "    print(\"\\n==================== END CHECK ====================\\n\")\n",
    "\n",
    "    return {\n",
    "        \"missing_queries\": missing_queries,\n",
    "        \"missing_rel_docs\": missing_rel_docs,\n",
    "        \"hybrid_docs_missing\": hybrid_docs_missing\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b397f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_RERANK = 100\n",
    "K_FINAL  = 10\n",
    "\n",
    "METHOD_NAME = \"Hybrid(tuned)+Rerank(amberoad-mBERT)\"\n",
    "RERANK_MODEL = \"amberoad/bert-multilingual-passage-reranking-msmarco\"\n",
    "RERANK_DEVICE = \"cpu\"\n",
    "RERANK_MAX_LEN = 256       # set in CrossEncoder\n",
    "DOC_TRUNC_CHARS = 400      # truncate documents to this length before reranking\n",
    "RERANK_BATCH = 8\n",
    "\n",
    "# Path of the shared, versionable file (to be pushed)\n",
    "FROZEN_DIR = \"results/rerank\"\n",
    "FROZEN_PATH = os.path.join(FROZEN_DIR, \"df_results_rerank_frozen.csv\")\n",
    "os.makedirs(FROZEN_DIR, exist_ok=True)\n",
    "\n",
    "FORCE_RERANK = False  # set True only if you want to recompute\n",
    "\n",
    "if (not FORCE_RERANK) and os.path.exists(FROZEN_PATH) and os.path.getsize(FROZEN_PATH) > 0:\n",
    "    df_results_rerank = pd.read_csv(FROZEN_PATH)\n",
    "    print(f\"Loaded frozen rerank results: {FROZEN_PATH}\")\n",
    "else:\n",
    "    print(\"Frozen rerank results not found (or FORCE_RERANK=True). Running reranking...\")\n",
    "\n",
    "    alpha_map = {\n",
    "        (r[\"split\"], r[\"language\"]): float(r[\"alpha\"])\n",
    "        for _, r in df_results_hybrid[df_results_hybrid[\"method\"]==\"Hybrid(tuned)\"].iterrows()\n",
    "    }\n",
    "\n",
    "    rerank_results = []\n",
    "\n",
    "    for split in CORPUS_SPLITS:\n",
    "        for lang_code in SELECTED_LANGUAGES:\n",
    "            key = (split, lang_code)\n",
    "            if key not in SCORE_CACHE:\n",
    "                continue\n",
    "\n",
    "            pack = SCORE_CACHE[key]\n",
    "            needed = [\"qrels\", \"bm25_topk_docs\", \"bm25_topk_scores\", \"dense_topk_docs\", \"dense_topk_scores\"]\n",
    "            if any(k not in pack for k in needed):\n",
    "                continue\n",
    "\n",
    "            alpha = alpha_map.get((split, lang_code), 0.3)\n",
    "            qrels = pack[\"qrels\"]\n",
    "            query_ids = list(qrels.keys())\n",
    "            query_ids = query_ids[:200]\n",
    "\n",
    "            lang_path = os.path.join(BASE_DATA_DIR, split, lang_code, \"train.jsonl\")\n",
    "\n",
    "            \n",
    "            # 1. Build set of needed document IDs\n",
    "            needed_ids = set()\n",
    "\n",
    "            for qid in query_ids:\n",
    "                qid = str(qid)\n",
    "                rel_doc = strip_language_prefix(qrels[qid])\n",
    "                needed_ids.add(rel_doc)\n",
    "\n",
    "                # top-K hybrid: ID with prefix -> clean\n",
    "                top_docs = hybrid_topk_for_query(\n",
    "                    qid=qid,\n",
    "                    alpha=alpha,\n",
    "                    pack=pack,\n",
    "                    K=K_RERANK,\n",
    "                    candN=CANDIDATE_N\n",
    "                )\n",
    "\n",
    "                clean = [strip_language_prefix(d) for d in top_docs]\n",
    "                needed_ids.update(clean)\n",
    "\n",
    "            print(f\"Needed documents: {len(needed_ids)}\")\n",
    "\n",
    "            # Load the exact documents from JSONL used for BM25/Dense\n",
    "            data = load_jsonl_robust(lang_path, max_items=MAX_ITEMS)\n",
    "\n",
    "            docs_map = {}\n",
    "            queries_map = {}\n",
    "\n",
    "            # Build maps with prefixed IDs\n",
    "            for item in data:\n",
    "                raw_id = str(item[\"_id\"])\n",
    "                doc_id = f\"{lang_code}_{raw_id}\"\n",
    "                # Only keep needed documents\n",
    "                queries_map[doc_id] = item.get(\"query\", \"\")\n",
    "                docs_map[doc_id] = (item.get(\"title\", \"\") + \" \" + item.get(\"text\", \"\")).strip()\n",
    "            \n",
    "            # Clean docs_map (without prefixes - clean IDs)\n",
    "            docs_map_clean = {\n",
    "                strip_language_prefix(k): truncate_text(v, max_chars=DOC_TRUNC_CHARS)\n",
    "                for k, v in docs_map.items()\n",
    "            }\n",
    "            \n",
    "            # Sanity check: ensure relevant doc is in docs_map_clean\n",
    "            allowed = set(docs_map_clean.keys())\n",
    "            example_qid = query_ids[0]\n",
    "            rel_clean = strip_language_prefix(qrels[example_qid])\n",
    "            print(\"Check overlap:\", rel_clean in allowed)\n",
    "            \n",
    "            # Clean queries map\n",
    "            queries_map_clean = {\n",
    "                strip_language_prefix(k): v\n",
    "                for k, v in queries_map.items()\n",
    "            }\n",
    "\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"RERANKING {split}/{lang_code} | alpha={alpha:.2f}\")\n",
    "            print(f\"queries loaded: {len(queries_map)} | docs loaded: {len(docs_map)}\")\n",
    "            print(f\"{'='*70}\")\n",
    "\n",
    "            # Prepare qrels for evaluation\n",
    "            qrels_eval = normalize_qrels_single_id(qrels)\n",
    "\n",
    "            # Sanity check\n",
    "            check = sanity_check(\n",
    "                queries_map=queries_map,\n",
    "                queries_map_clean=queries_map_clean,\n",
    "                docs_map_clean=docs_map_clean,\n",
    "                qrels=qrels,\n",
    "                pack=pack,\n",
    "                alpha=alpha,\n",
    "                K_RERANK=K_RERANK,\n",
    "                CANDIDATE_N=CANDIDATE_N\n",
    "            )\n",
    "\n",
    "            reranked_runs = {}\n",
    "            t0_all = time.time()\n",
    "\n",
    "            # Rerank each query\n",
    "            for i, qid_original in enumerate(query_ids):\n",
    "                qid_original = str(qid_original)\n",
    "                qid_clean = strip_language_prefix(qid_original)\n",
    "\n",
    "                qtext = queries_map.get(qid_original) or queries_map_clean.get(qid_clean)\n",
    "                if not qtext:\n",
    "                    reranked_runs[qid_original] = []\n",
    "                    continue\n",
    "\n",
    "                # Stage 1: HYBRID to get candidates\n",
    "                top_docs = hybrid_topk_for_query(\n",
    "                    qid=qid_original,\n",
    "                    alpha=alpha,\n",
    "                    pack=pack,\n",
    "                    K=K_RERANK,\n",
    "                    candN=CANDIDATE_N\n",
    "                )\n",
    "\n",
    "                if not top_docs:\n",
    "                    reranked_runs[qid_original] = []\n",
    "                    continue\n",
    "\n",
    "                # Clean IDs to match docs_map format (WITHOUT prefix)\n",
    "                top_docs_clean = [strip_language_prefix(d) for d in top_docs]\n",
    "                valid_docs = [d for d in top_docs_clean if d in docs_map_clean]\n",
    "                print(f\"Query {qid_original}: {len(valid_docs)} documenti validi per rerank\")\n",
    "\n",
    "                if len(valid_docs) > 0:\n",
    "                    print(f\"  Primo doc: {valid_docs[0]}, testo: {len(docs_map_clean.get(valid_docs[0], ''))} chars\")\n",
    "\n",
    "                if not valid_docs:\n",
    "                    # Fallback su BM25: use only BM25 candidates that are valid\n",
    "                    bm25_docs = pack[\"bm25_topk_docs\"].get(qid_original, [])\n",
    "                    bm25_docs_clean = [strip_language_prefix(d) for d in bm25_docs]\n",
    "                    valid_docs = [d for d in bm25_docs_clean if d in docs_map_clean]\n",
    "\n",
    "                if not valid_docs:\n",
    "                    # if still no valid docs, skip\n",
    "                    reranked_runs[qid_original] = []\n",
    "                    continue\n",
    "\n",
    "                # verify if documents are valid\n",
    "                if len(valid_docs) == 0:\n",
    "                    reranked_runs[qid_original] = []\n",
    "                    continue\n",
    "\n",
    "                # Stage 2: Rerank (CPU-friendly, small batches)\n",
    "                rr_docs, rr_scores = rerank_topk_crossencoder(\n",
    "                    reranker=reranker,\n",
    "                    query_text=qtext,\n",
    "                    doc_ids=valid_docs,\n",
    "                    documents=docs_map_clean,\n",
    "                    batch_size=8,\n",
    "                    show_progress=False\n",
    "                )\n",
    "\n",
    "                # If no reranked docs, skip\n",
    "                if not rr_docs:\n",
    "                    reranked_runs[qid_original] = []\n",
    "                    continue\n",
    "                # Keep only top-K_FINAL\n",
    "                reranked_runs[qid_original] = rr_docs[:K_FINAL]\n",
    "\n",
    "                # Print every 10 query for debugging\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"Query {qid_original}: {len(valid_docs)} documenti → {len(rr_docs)} dopo rerank\")\n",
    "                    if rr_scores:\n",
    "                        print(f\"  Score range: [{min(rr_scores):.4f}, {max(rr_scores):.4f}]\")\n",
    "                        print(f\"  Top-3 scores: {rr_scores[:3]}\")\n",
    "                else:\n",
    "                    print(f\"Query {qid_original}: top score = {rr_scores[0]:.4f}\" if rr_scores else f\"Query {qid_original}: no scores\")\n",
    "\n",
    "                # Log every 50 queries for execution time\n",
    "                if (i + 1) % 50 == 0:\n",
    "                    elapsed = time.time() - t0_all\n",
    "                    print(f\"[{i+1}/{len(query_ids)}] elapsed: {elapsed/60:.1f} min\")\n",
    "                    print(\"-\" * 50)\n",
    "\n",
    "            # Evaluate\n",
    "            recall_r, mrr_r = evaluate_dictqrels(\n",
    "                reranked_runs, qrels_eval, K=K_FINAL\n",
    "            )\n",
    "\n",
    "            print(f\"\\n>>> RESULT {split}/{lang_code}\")\n",
    "            print(f\"Recall@10 = {recall_r:.4f}\")\n",
    "            print(f\"MRR@10    = {mrr_r:.4f}\")\n",
    "\n",
    "            # Store results\n",
    "            rerank_results.append({\n",
    "                \"split\": split,\n",
    "                \"language\": lang_code,\n",
    "                \"Recall@10\": float(recall_r),\n",
    "                \"MRR@10\": float(mrr_r),\n",
    "                \"method\": METHOD_NAME,\n",
    "                \"alpha\": float(alpha),\n",
    "                \"candN\": int(CANDIDATE_N),\n",
    "                \"rerankK\": int(K_RERANK),\n",
    "\n",
    "                # metadata useful for reproducibility\n",
    "                \"rerank_model\": RERANK_MODEL,\n",
    "                \"device\": RERANK_DEVICE,\n",
    "                \"max_length\": int(RERANK_MAX_LEN),\n",
    "                \"doc_trunc_chars\": int(DOC_TRUNC_CHARS),\n",
    "                \"batch_size\": int(RERANK_BATCH),\n",
    "            })\n",
    "\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            df_results_rerank = pd.DataFrame(rerank_results)\n",
    "            df_results_rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the rerank results\n",
    "\n",
    "FROZEN_DIR = \"results/rerank\"\n",
    "FROZEN_PATH = os.path.join(FROZEN_DIR, \"df_results_rerank_frozen.csv\")\n",
    "os.makedirs(FROZEN_DIR, exist_ok=True)\n",
    "\n",
    "if \"df_results_rerank\" not in globals() or df_results_rerank is None or df_results_rerank.empty:\n",
    "    raise RuntimeError(\"df_results_rerank not available or empty. Run the rerank cell first.\")\n",
    "\n",
    "df_results_rerank.to_csv(FROZEN_PATH, index=False)\n",
    "print(f\" Saved frozen rerank results to: {FROZEN_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"results/rerank/df_results_rerank_frozen.csv\"  \n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "cols_preferred = [\n",
    "    \"split\", \"language\", \"Recall@10\", \"MRR@10\", \"alpha\", \"candN\", \"rerankK\", \"device\", \"max_length\", \"doc_trunc_chars\", \"batch_size\", \"method\"\n",
    "]\n",
    "cols = [c for c in cols_preferred if c in df.columns]\n",
    "df_view = df[cols].copy() if cols else df.copy()\n",
    "\n",
    "sort_cols = [c for c in [\"split\", \"language\"] if c in df_view.columns]\n",
    "if \"MRR@10\" in df_view.columns:\n",
    "    df_view = df_view.sort_values(by=sort_cols + [\"MRR@10\"], ascending=[True]*len(sort_cols) + [False])\n",
    "else:\n",
    "    df_view = df_view.sort_values(by=sort_cols) if sort_cols else df_view\n",
    "\n",
    "display(df_view)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e3ac7",
   "metadata": {},
   "source": [
    "# Results and Conclusions\n",
    "\n",
    "## Summary of Results\n",
    "\n",
    "Our experiments comparing BM25 (sparse retrieval), LaBSE (dense retrieval), and Hybrid methods across 5 languages (German, French, Spanish, English, Chinese) in both monolingual and cross-lingual settings yielded clear findings:\n",
    "\n",
    "### Overall Performance\n",
    "- **LaBSE Recall@10**: 84.9% (vs. BM25: 66.9%)\n",
    "- **LaBSE MRR@10**: 0.708 (vs. BM25: 0.575)\n",
    "- **Hybrid Recall@10**: 87.9% (best overall)\n",
    "- **Hybrid MRR@10**: 0.761 (best overall)\n",
    "- **Head-to-head**: LaBSE wins 7 out of 8 language comparisons against BM25 (87.5%)\n",
    "\n",
    "### Language-Specific Performance\n",
    "\n",
    "**Monolingual Results:**\n",
    "- **German**: LaBSE (88.8% Recall, 0.799 MRR) significantly outperforms BM25\n",
    "- **English**: LaBSE (93.0% Recall, 0.846 MRR) - highest performance across all languages\n",
    "- **Spanish**: LaBSE (86.1% Recall, 0.734 MRR) shows strong semantic understanding\n",
    "- **French**: LaBSE (76.1% Recall, 0.616 MRR) - competitive with BM25\n",
    "\n",
    "**Cross-Lingual Results:**\n",
    "- **Chinese**: LaBSE (85.4% Recall, 0.679 MRR) - crucial for non-alphabetic systems\n",
    "- **German**: LaBSE (85.2% Recall, 0.684 MRR) - consistent cross-lingual performance\n",
    "- **French**: LaBSE (79.4% Recall, 0.626 MRR) - handles language mismatch well\n",
    "- **Spanish**: LaBSE (85.3% Recall, 0.678 MRR) - strong multilingual alignment\n",
    "\n",
    "### Hybrid Method Performance\n",
    "\n",
    "The tuned hybrid approach (combining BM25 and LaBSE with optimized α parameter) achieved:\n",
    "- **German (monolingual)**: 92.5% Recall, 0.875 MRR with α=0.30\n",
    "- **Spanish (monolingual)**: 90.5% Recall, 0.826 MRR with α=0.30\n",
    "- **English (monolingual)**: 95.0% Recall, 0.898 MRR with α=0.20\n",
    "\n",
    "The α parameter varied by language (0.20-0.50), indicating language-specific optimal balances between lexical and semantic matching.\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Cross-lingual superiority**: LaBSE dramatically outperforms BM25 on cross-lingual tasks\n",
    "   - BM25 struggles when query and document are in different languages (limited by vocabulary mismatch)\n",
    "   - LaBSE's multilingual embeddings enable effective cross-lingual matching through semantic alignment\n",
    "   - All cross-lingual tasks showed >79% Recall@10 with LaBSE\n",
    "\n",
    "2. **Monolingual performance**: LaBSE excels even in monolingual scenarios\n",
    "   - Contrary to expectations, LaBSE wins on English monolingual (93% vs BM25's lower performance)\n",
    "   - Semantic matching captures paraphrases and synonyms that lexical matching misses\n",
    "   - Only competitive on French monolingual, where both methods perform similarly\n",
    "\n",
    "3. **Hybrid advantage**: Combining methods yields best results\n",
    "   - Hybrid consistently outperforms both individual methods\n",
    "   - Language-specific tuning of α shows importance of balancing lexical and semantic signals\n",
    "   - Lower α values (favoring dense retrieval) work better for Germanic languages\n",
    "   - Higher α values (more BM25 weight) needed for Romance languages\n",
    "\n",
    "4. **Language-specific insights**:\n",
    "   - **Largest improvements**: English (monolingual) shows highest absolute performance\n",
    "   - **Cross-lingual strength**: Chinese benefits most from semantic matching (no alphabet overlap with English queries)\n",
    "   - **Consistent performance**: German shows stable high performance across both splits\n",
    "   - **Optimization opportunities**: French and Spanish show room for improvement with better α tuning\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "Dense retrieval using multilingual embeddings (LaBSE) significantly outperforms traditional sparse methods (BM25) for multilingual information retrieval in both monolingual and cross-lingual scenarios. The ability to capture semantic similarity across languages makes dense retrieval the strongly preferred approach for multilingual IR systems.\n",
    "\n",
    "The hybrid approach combining BM25 and LaBSE scores with tuned fusion weights achieves the best overall performance (87.9% Recall@10, 0.761 MRR@10), demonstrating that lexical and semantic signals complement each other effectively. Language-specific α tuning is crucial, as optimal balance varies significantly across language families.\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "1. **For production systems**: Deploy hybrid methods with language-specific tuning for optimal results\n",
    "2. **For cross-lingual search**: LaBSE alone provides strong baseline (85%+ recall)\n",
    "3. **For monolingual English**: Hybrid with low α (0.2-0.3) maximizes performance\n",
    "4. **For resource-constrained settings**: LaBSE-only provides excellent tradeoff between performance and complexity\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- **Dataset size**: Evaluated on 1,000 documents per language (limited by memory constraints on M3 Mac with 16GB RAM)\n",
    "- **Language coverage**: Only 5 languages tested from the available 40+ in SWIM-IR dataset\n",
    "- **Hardware constraints**: Memory limitations prevented larger batch sizes and full dataset evaluation; used batch_size=1 for encoding\n",
    "- **Single dense model**: Only evaluated LaBSE; other multilingual models (mE5, mMiniLM) might show different performance characteristics\n",
    "- **Evaluation setup**: Each query has exactly one relevant document; real-world scenarios may have multiple relevant documents\n",
    "- **Validation set size**: Limited validation data (20% split) may affect reliability of α tuning\n",
    "\n",
    "## Future Work\n",
    "\n",
    "1. **Extended language coverage**: Evaluate on all 40+ languages in SWIM-IR, including low-resource languages\n",
    "2. **Alternative fusion methods**: Explore Reciprocal Rank Fusion (RRF) and learned-to-rank approaches\n",
    "3. **Query expansion**: Investigate multilingual query expansion techniques\n",
    "4. **Fine-tuning**: Adapt LaBSE on domain-specific multilingual data for specialized applications\n",
    "5. **Efficiency optimization**: Implement FAISS or approximate nearest neighbor methods for larger-scale deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_bello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
